{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BmENsxfVNFes"
      },
      "outputs": [],
      "source": [
        "import cmath\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-7CdB3LZNMOt"
      },
      "outputs": [],
      "source": [
        "def createFourierMatrix(k,n):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    i = cmath.sqrt(-1)\n",
        "    val = cmath.exp(-2*cmath.pi*i/n)\n",
        "    p = (k-1)/2\n",
        "    q = (k+1)/2\n",
        "    F = torch.zeros(n*n,k*k).to(device)\n",
        "    F = F.type(torch.complex64)\n",
        "\n",
        "    f = torch.zeros(n,1).to(device)\n",
        "    f = f.type(torch.complex64)\n",
        "    f_u = torch.zeros(n*n,1).to(device)\n",
        "    f_u = f_u.type(torch.complex64)\n",
        "    for u in range(n):\n",
        "        f_u[u*n:(u+1)*n]=val**u\n",
        "        f[u]=val**u;\n",
        "\n",
        "    f_v = f.repeat(n,1);\n",
        "    for u in range(k):\n",
        "        for v in range(k):\n",
        "            a=0\n",
        "            b=0\n",
        "            if(u<=p):\n",
        "                a = n-p+u;\n",
        "            else:\n",
        "                a = u-p;\n",
        "\n",
        "\n",
        "            if(v<=p):\n",
        "                b = n-p+v;\n",
        "            else:\n",
        "                b = v-p;\n",
        "\n",
        "            F[:,(u*k+v)]=((torch.pow(f_u,(a)))*(torch.pow(f_v,(b)))).flatten();\n",
        "\n",
        "    return F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i5mlF5gVNRFt"
      },
      "outputs": [],
      "source": [
        "def zeroPad2DMatrix(layer_wt,n):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    k = layer_wt.size()[3]\n",
        "    p = (k-1)/2\n",
        "    q = (k+1)/2\n",
        "    I = torch.eye(n).to(device);\n",
        "    ind1 = torch.arange(0,p)\n",
        "    ind2 = torch.arange(p,k)\n",
        "    ind3 = torch.arange(k,n)\n",
        "    indices = torch.cat((ind2,ind3,ind1))\n",
        "    indices=indices.type(torch.int64)\n",
        "    perm = I[indices].to(device)\n",
        "    perm_mat = perm.unsqueeze(0).unsqueeze(0)\n",
        "    pad_left = 0\n",
        "    pad_right = n - k\n",
        "    pad_top = 0\n",
        "    pad_bottom = n - k\n",
        "    # Apply padding\n",
        "    padded_wt = torch.nn.functional.pad(layer_wt, (pad_left, pad_right, pad_top, pad_bottom)).to(device)\n",
        "    perm_mat_tr = torch.transpose(perm_mat,2,3)\n",
        "    padded_final = torch.matmul(perm_mat,torch.matmul(padded_wt,perm_mat_tr))\n",
        "    return padded_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def deZeroPad2DMatrix(layer_wt,k):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    n = layer_wt.size()[-1]\n",
        "    p = (k-1)/2\n",
        "    q = (k+1)/2\n",
        "    I = torch.eye(n).to(device);\n",
        "    ind1 = torch.arange(0,p)\n",
        "    ind2 = torch.arange(p,k)\n",
        "    ind3 = torch.arange(k,n)\n",
        "    indices = torch.cat((ind2,ind3,ind1))\n",
        "    indices=indices.type(torch.int64)\n",
        "    perm = I[indices].to(device)\n",
        "    #perm_mat = perm.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply padding\n",
        "    perm_mat_tr = torch.transpose(perm,-2,-1)\n",
        "\n",
        "    unpadded_wt = torch.matmul(perm_mat_tr,torch.matmul(layer_wt,perm))\n",
        "\n",
        "    layer = unpadded_wt[:,:,:k,:k].to(device)\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1MCbBIBcNTaH"
      },
      "outputs": [],
      "source": [
        "def computeLayerLipschitzFourier(layer_wt,n):\n",
        "    layer_wt_padded = zeroPad2DMatrix(layer_wt,n)\n",
        "    layer_pf=torch.fft.fft2(layer_wt_padded)\n",
        "    layer_fperm = torch.permute(layer_pf,(2,3,0,1))\n",
        "    sing = torch.linalg.svdvals(layer_fperm)\n",
        "    lip = torch.max(torch.abs(sing))\n",
        "    return lip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGm1luP7NVeL",
        "outputId": "72e97eb5-71c9-44f5-d633-f8b8d1f3d844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "alexnet_model = models.alexnet(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYgvgHcZNa44",
        "outputId": "801f79c0-540c-460e-b2e8-650da3b0be04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(14.6738, device='cuda:0', grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "layer = alexnet_model.features[8].weight.to(device)\n",
        "lip = computeLayerLipschitzFourier(layer,13)\n",
        "print(lip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fBBYU6IXNp_e"
      },
      "outputs": [],
      "source": [
        "def createComplexVerticesL2Norm(dim):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    vertices = torch.empty((dim, 0), dtype=torch.complex64).to(device)\n",
        "    I_dim = torch.eye(dim, dtype=torch.complex64).to(device)\n",
        "    vertices = torch.cat((I_dim, -I_dim), dim=1)\n",
        "    return vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYEapSS9O3nN",
        "outputId": "7f757cd5-74ad-41a5-d4ba-194a3e9c0236"
      },
      "outputs": [],
      "source": [
        "vertices = createComplexVerticesL2Norm(layer.shape[0]*layer.shape[1])\n",
        "print(vertices.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zjk32yzKPSj8"
      },
      "outputs": [],
      "source": [
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2hT-Jt4PnPL",
        "outputId": "a8997446-b6ce-4949-96e6-017affd68235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "Step 0, loss = 19673526272.0\n",
            "Step 100, loss = 82443960.0\n",
            "Step 200, loss = 41432460.0\n",
            "Step 300, loss = 41801620.0\n",
            "Step 400, loss = 36457856.0\n",
            "Step 500, loss = 43181500.0\n",
            "Step 600, loss = 39966356.0\n",
            "Step 700, loss = 41805592.0\n",
            "Step 800, loss = 40656724.0\n",
            "Step 900, loss = 41811604.0\n",
            "Optimized H:\n",
            "tensor([[ 0.0106,  0.0100, -0.0029,  ..., -0.0029,  0.0100,  0.0106],\n",
            "        [-0.0083, -0.0044,  0.0015,  ...,  0.0015, -0.0044, -0.0083],\n",
            "        [-0.0037,  0.0020,  0.0030,  ...,  0.0030,  0.0020, -0.0037],\n",
            "        ...,\n",
            "        [-0.0052, -0.0023, -0.0026,  ..., -0.0026, -0.0023, -0.0052],\n",
            "        [ 0.0022,  0.0055, -0.0002,  ..., -0.0002,  0.0055,  0.0022],\n",
            "        [-0.0126,  0.0063,  0.0002,  ...,  0.0002,  0.0063, -0.0126]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Optimized lambda:\n",
            "tensor([[ 0.0796,  0.2529, -0.3796,  ..., -0.1417, -0.3499, -0.4002],\n",
            "        [-0.3885, -0.3099, -0.1370,  ...,  0.3589, -0.0375,  0.2595],\n",
            "        [-0.2342, -0.4731, -0.0374,  ...,  0.1264,  0.2079,  0.2391],\n",
            "        ...,\n",
            "        [ 0.4619, -0.2372,  0.2915,  ..., -0.4817,  0.3334,  0.2473],\n",
            "        [ 0.0794,  0.2853,  0.2676,  ..., -0.4691,  0.4872,  0.0768],\n",
            "        [ 0.4080,  0.2896,  0.0477,  ..., -0.2812, -0.3966, -0.4348]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#Polytope Projection\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Constants (example values, replace these with actual data)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "V = vertices.to(device)\n",
        "F = F.to(device)\n",
        "V_real, V_imag = torch.real(V), torch.imag(V)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "print(F_real.dtype)\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "lambda_ = torch.rand((V.shape[1], n*n), requires_grad=True,dtype = torch.float32,device=device)  # size based on V and H0\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H, lambda_], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 1000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    eq_constraint_real = V_real @ lambda_ - H@F_real.T\n",
        "    eq_constraint_imag = V_imag @ lambda_ - H@F_imag.T\n",
        "    eq_constraint = torch.sqrt(eq_constraint_real**2 + eq_constraint_imag**2)\n",
        "    loss = loss+ penalty_weight_eq * torch.norm(eq_constraint,p='fro')  # Penalty for the equality constraint\n",
        "\n",
        "    # Column sum constraint: sum of each column of lambda = 1\n",
        "    col_sum_constraint = torch.abs(torch.sum(lambda_, dim=0) - 1)\n",
        "    loss = loss + penalty_weight_eq * torch.sum(col_sum_constraint)  # Penalty for the column sum constraint\n",
        "\n",
        "    # Inequality constraint: lambda >= 0\n",
        "    ineq_constraint = torch.norm(torch.nn.functional.relu(lambda_) - lambda_,p = 'fro')\n",
        "    loss = loss + penalty_weight_eq * torch.sum(ineq_constraint)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')\n",
        "print(f'Optimized lambda:\\n{lambda_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(40949.4922, device='cuda:0', grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.sum(torch.abs(torch.sum(lambda_, dim=0) - 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(26.4691, grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = torch.norm(H - H0, p='fro')\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1367, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "layer_wt = layer_wt.to('cpu')\n",
        "print(computeLayerLipschitzFourier(layer_wt,40))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, loss = 3501525760.0\n",
            "Step 100, loss = 90510584.0\n",
            "Step 200, loss = 2879210.75\n",
            "Step 300, loss = 30224.744140625\n",
            "Step 400, loss = 26.170150756835938\n",
            "Step 500, loss = 26.17002296447754\n",
            "Step 600, loss = 26.169885635375977\n",
            "Step 700, loss = 26.16973304748535\n",
            "Step 800, loss = 26.16956329345703\n",
            "Step 900, loss = 26.169376373291016\n",
            "Optimized H:\n",
            "tensor([[-2.5685e-05, -3.4364e-05, -6.8903e-05,  ..., -4.7072e-05,\n",
            "         -2.3813e-05, -1.6826e-05],\n",
            "        [-2.9484e-06, -4.2947e-07, -1.2389e-06,  ..., -1.1497e-06,\n",
            "         -2.2666e-06,  7.3690e-07],\n",
            "        [-2.7154e-04, -6.3394e-04, -9.6890e-04,  ...,  2.4672e-03,\n",
            "         -6.1208e-04, -3.6221e-04],\n",
            "        ...,\n",
            "        [-3.3118e-05, -3.0584e-05, -7.6941e-06,  ...,  5.3623e-05,\n",
            "         -5.2775e-05, -5.9902e-05],\n",
            "        [-1.1631e-04, -1.3938e-04, -1.0898e-04,  ..., -8.2066e-05,\n",
            "         -5.7592e-05, -2.0274e-05],\n",
            "        [ 6.9687e-06,  1.6077e-06, -3.3218e-07,  ..., -8.5423e-07,\n",
            "         -5.2531e-06,  2.4060e-06]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#Frobenius Norm Projection - Constraint with RELU\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "F = F.to(device)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 1000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    ineq_constraint_real = H@F_real.T\n",
        "    ineq_constraint_imag = H@F_imag.T\n",
        "    ineq_constraint_sum = 1 - torch.sum((ineq_constraint_real**2 + ineq_constraint_imag**2),dim=0)\n",
        "\n",
        "    # Inequality constraint: lambda >= 0\n",
        "    ineq_constraint = torch.norm(torch.nn.functional.relu(ineq_constraint_sum) - ineq_constraint_sum,p = 'fro')\n",
        "    loss = loss + penalty_weight_ineq * torch.sum(ineq_constraint)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.384726345539093\n",
            "Difference:26.16917610168457\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p='fro')}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, loss = 852986304.0\n"
          ]
        }
      ],
      "source": [
        "# Bhartendu with loop - FAILED\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "F = F.to(device)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 1000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    Hf_real = H@F_real.T\n",
        "    Hf_imag = H@F_imag.T\n",
        "    Hf = torch.sqrt(Hf_real**2 + Hf_imag**2)\n",
        "    for i in range(n*n):\n",
        "        mat = torch.reshape(Hf[:,i],(s[0],s[1]))\n",
        "        sum_col = 1-torch.sum(mat,dim=0)\n",
        "        sum_row = 1-torch.sum(mat,dim=1)\n",
        "        loss = loss +penalty_weight_ineq* torch.norm(torch.nn.functional.relu(sum_col) - sum_col,p = 'fro')\n",
        "        loss = loss + penalty_weight_ineq*torch.norm(torch.nn.functional.relu(sum_row) - sum_row,p = 'fro')\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, loss = 70200131584.0\n",
            "Step 100, loss = 1816661632.0\n",
            "Step 200, loss = 57875504.0\n",
            "Step 300, loss = 605619.5\n",
            "Step 400, loss = 26.17026138305664\n",
            "Step 500, loss = 26.170249938964844\n",
            "Step 600, loss = 26.170242309570312\n",
            "Step 700, loss = 26.17023468017578\n",
            "Step 800, loss = 26.170228958129883\n",
            "Step 900, loss = 26.17021942138672\n",
            "Optimized H:\n",
            "tensor([[-2.9761e-05, -4.5923e-05, -1.1979e-04,  ..., -2.6065e-04,\n",
            "          7.2066e-04, -2.4547e-05],\n",
            "        [-1.0144e-04,  7.1570e-04, -1.6990e-04,  ...,  1.1148e-03,\n",
            "          1.2668e-03,  1.3764e-04],\n",
            "        [-1.4357e-04, -2.8224e-04, -1.1838e-04,  ..., -5.9225e-04,\n",
            "         -4.2574e-04, -2.1453e-04],\n",
            "        ...,\n",
            "        [-3.5816e-05, -5.8492e-05, -3.4246e-05,  ..., -1.1027e-04,\n",
            "          1.9379e-04,  2.9805e-05],\n",
            "        [-4.5587e-04,  1.3412e-03,  3.9402e-03,  ..., -1.2584e-03,\n",
            "         -9.3011e-04, -6.2155e-04],\n",
            "        [-7.8389e-05, -1.0949e-04, -7.2908e-05,  ..., -2.7899e-05,\n",
            "         -1.1135e-05, -2.1568e-06]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#Frobenius Norm Projection - Constraint with abs\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "F = F.to(device)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 10000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    ineq_constraint_real = H@F_real.T\n",
        "    ineq_constraint_imag = H@F_imag.T\n",
        "    ineq_constraint_sum = 1 - torch.sum((ineq_constraint_real**2 + ineq_constraint_imag**2),dim=0)\n",
        "\n",
        "    # Inequality constraint: lambda >= 0\n",
        "    ineq_constraint = torch.norm(torch.abs(ineq_constraint_sum) - ineq_constraint_sum,p = 'fro')\n",
        "    loss = loss + penalty_weight_ineq * torch.sum(ineq_constraint)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.3922692835330963\n",
            "Difference:26.170207977294922\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p='fro')}')\n",
        "print(f'Original H:\\n{H0}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, loss = 4154583.25\n",
            "Step 100, loss = 43754.02734375\n",
            "Step 200, loss = 1723.7564697265625\n",
            "Step 300, loss = 1105.1204833984375\n",
            "Step 400, loss = 671.1167602539062\n",
            "Step 500, loss = 508.1769714355469\n",
            "Step 600, loss = 366.5533142089844\n",
            "Step 700, loss = 914.8499145507812\n",
            "Step 800, loss = 661.396728515625\n",
            "Step 900, loss = 1072.9964599609375\n",
            "Optimized H:\n",
            "tensor([[ 5.9885e-04, -6.3687e-04,  1.4169e-03,  ...,  1.4092e-03,\n",
            "          2.9666e-04, -7.8557e-04],\n",
            "        [ 5.8077e-04, -8.6543e-04,  8.2217e-05,  ...,  2.3715e-04,\n",
            "          1.7002e-04, -1.8742e-04],\n",
            "        [-8.1851e-05, -6.9085e-04, -1.1079e-04,  ...,  3.7614e-05,\n",
            "         -4.7068e-05,  5.5482e-04],\n",
            "        ...,\n",
            "        [-9.3642e-05, -1.1380e-03,  1.1766e-03,  ..., -8.5665e-05,\n",
            "         -1.4870e-04,  2.2091e-04],\n",
            "        [-3.1953e-04,  7.8229e-04,  2.5259e-03,  ...,  7.2739e-05,\n",
            "         -1.2424e-05, -3.6301e-04],\n",
            "        [ 1.2187e-03,  1.7081e-04,  3.1731e-04,  ...,  1.0625e-03,\n",
            "          7.7116e-05,  2.7824e-04]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#Bhartendu Constraint - Projected on Constraints\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "F = F.to(device)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 1000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    Hf_real = H@F_real.T\n",
        "    Hf_imag = H@F_imag.T\n",
        "    Hf = torch.sqrt(Hf_real**2 + Hf_imag**2)\n",
        "    H_mat = torch.permute(torch.reshape(Hf,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "\n",
        "    H1_norm = torch.clamp(torch.sum(H_mat, dim=2),min=1.0)\n",
        "    s_h1 = H1_norm.shape;\n",
        "    H_mat_1_norm = torch.div(H_mat,torch.reshape(H1_norm,(s_h1[0],s_h1[1],1,s_h1[2])))\n",
        "    Hinf_norm = torch.clamp(torch.sum(H_mat_1_norm, dim=3),min=1.0)\n",
        "    s_hinf = Hinf_norm.shape;\n",
        "    H_mat_norm = torch.div(H_mat_1_norm,torch.reshape(Hinf_norm,(s_hinf[0],s_hinf[1],s_hinf[2],1)))\n",
        "    \n",
        "    loss = loss + penalty_weight_ineq*(torch.norm((H_mat-H_mat_norm),p='fro'))\n",
        "\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 2.309136390686035\n",
            "Difference:26.436567306518555\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p='fro')}')\n",
        "print(f'Original H:\\n{H0}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Bhartendu Constraint No Loop - FAILED\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "F = F.to(device)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "\n",
        "# Penalty weights\n",
        "penalty_weight_eq = 1000.0\n",
        "penalty_weight_ineq = 10000.0\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(4000):\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Objective function: Frobenius norm of (H - H0)\n",
        "    loss = torch.norm(H - H0, p='fro')*0.1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    \n",
        "    # Print progress\n",
        "    if step % 100 == 0:\n",
        "        print(f'Step {step}, loss = {loss.item()}')\n",
        "\n",
        "    # Equality constraint: V * lambda = H * F\n",
        "    Hf_real = H@F_real.T\n",
        "    Hf_imag = H@F_imag.T\n",
        "    Hf = torch.sqrt(Hf_real**2 + Hf_imag**2)\n",
        "    H_mat = torch.permute(torch.reshape(Hf,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "    \n",
        "    H1_norm = 1-torch.max(torch.sum(H_mat, dim=2))\n",
        "    Hinf_norm = 1-torch.max(torch.sum(H_mat, dim=3))\n",
        "    loss = loss + penalty_weight_ineq*(torch.norm((torch.abs(H1_norm)-H1_norm),p='fro'))\n",
        "    loss = loss + penalty_weight_ineq*(torch.norm((torch.abs(Hinf_norm)-Hinf_norm),p='fro'))\n",
        "\n",
        "    \n",
        "\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.9720, 0.5755, 0.5245, 0.9264],\n",
            "         [0.3239, 0.5982, 0.4861, 0.7159],\n",
            "         [0.8257, 0.7799, 0.8597, 0.6664]],\n",
            "\n",
            "        [[0.7571, 0.6749, 0.7261, 0.8427],\n",
            "         [0.8832, 0.8098, 0.9694, 0.1520],\n",
            "         [0.4983, 0.1817, 0.6956, 0.5596]]])\n"
          ]
        }
      ],
      "source": [
        "r = torch.rand(2,3,4)\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.6868, 0.9763, 1.7824, 1.3842],\n",
            "        [1.9146, 2.0612, 0.8041, 1.5873]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[0.3349, 0.3453, 0.4519, 0.5676],\n",
              "         [0.0794, 0.0190, 0.3525, 0.0844],\n",
              "         [0.5857, 0.6356, 0.1956, 0.3481]],\n",
              "\n",
              "        [[0.3671, 0.3947, 0.0061, 0.5868],\n",
              "         [0.4346, 0.2387, 0.3102, 0.0871],\n",
              "         [0.1983, 0.3666, 0.6837, 0.3261]]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = torch.sum(r,dim=1)\n",
        "print(temp)\n",
        "temp = torch.reshape(temp,(2,1,4))\n",
        "torch.div(r,temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.9983, 2.1242, 3.1318],\n",
            "        [3.0008, 2.8145, 1.9352]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[0.3242, 0.1919, 0.1749, 0.3090],\n",
              "         [0.1525, 0.2816, 0.2288, 0.3370],\n",
              "         [0.2637, 0.2490, 0.2745, 0.2128]],\n",
              "\n",
              "        [[0.2523, 0.2249, 0.2420, 0.2808],\n",
              "         [0.3138, 0.2877, 0.3444, 0.0540],\n",
              "         [0.2575, 0.0939, 0.3594, 0.2892]]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = torch.sum(r,dim=2)\n",
        "print(temp)\n",
        "temp = torch.reshape(temp,(2,3,1))\n",
        "torch.div(r,temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.+1.j, 0.+1.j])\n"
          ]
        }
      ],
      "source": [
        "temp = torch.ones(2)\n",
        "temp2 = torch.zeros(2,dtype=torch.complex64)\n",
        "temp2.imag = temp\n",
        "print(temp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0.0000, 0.3000, 0.0000],\n",
            "          [0.0000, 0.4000, 0.0000],\n",
            "          [0.0000, 0.3000, 0.0000]]]])\n"
          ]
        }
      ],
      "source": [
        "layer = torch.tensor([[[[0,0.3,0],[0,0.4,0],[0,0.3,0]]]])\n",
        "print(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer.shape\n",
        "computeLayerLipschitzFourier(layer,13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SGDminimize(X0,Z,U,F,rho):\n",
        "    F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X = torch.rand_like(X0, requires_grad=True,dtype = torch.float32).to(device)\n",
        "    optimizer = torch.optim.Adam([X], lr=0.01)\n",
        "    for step in range(1000):\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.norm(X - X0, p='fro')**2\n",
        "        res_real = X@F_real.T - Z.real + U.real\n",
        "        res_imag = X@F_imag.T - Z.imag + U.imag \n",
        "        loss = loss + (rho/2)*torch.sum((res_imag**2 + res_real**2))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #if(step%100 ==0):\n",
        "        #    print(loss)\n",
        "    \n",
        "    return X.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([36, 9])\n",
            "torch.Size([169, 9])\n",
            "torch.Size([36, 9])\n",
            "Step 0: Primal Residual:6.122568130493164\n",
            "Step 10: Primal Residual:0.3624890446662903\n",
            "Step 20: Primal Residual:0.21123899519443512\n",
            "Step 30: Primal Residual:0.13808706402778625\n",
            "Step 40: Primal Residual:0.0900331363081932\n",
            "Step 50: Primal Residual:0.05904947593808174\n",
            "Step 60: Primal Residual:0.0378074124455452\n",
            "Step 70: Primal Residual:0.025564517825841904\n",
            "Step 80: Primal Residual:0.017265474423766136\n",
            "Step 90: Primal Residual:0.01165198627859354\n",
            "Optimized H:\n",
            "tensor([[-7.1758e-02,  3.2251e-02,  9.8408e-02,  1.0451e-02, -2.7271e-02,\n",
            "          5.6846e-02, -4.0494e-02, -2.4730e-02,  1.1890e-01],\n",
            "        [-1.2736e-02, -2.5142e-02,  6.5172e-02,  6.3220e-02, -8.2236e-02,\n",
            "         -5.6564e-03,  9.6499e-02,  5.6072e-02,  5.2354e-02],\n",
            "        [ 8.6255e-02, -5.5742e-02, -1.6814e-02, -3.3595e-02, -3.7666e-02,\n",
            "         -5.2177e-02, -2.5699e-03,  1.2814e-01,  1.1503e-01],\n",
            "        [ 2.6714e-02, -9.6533e-03,  8.3881e-02,  5.8944e-02, -4.7699e-02,\n",
            "          1.4396e-02,  1.0084e-02,  4.1413e-02,  3.2520e-02],\n",
            "        [ 1.0008e-01,  4.2038e-03,  5.7550e-02,  5.0623e-03, -6.2533e-02,\n",
            "          4.4771e-02, -1.1952e-02,  5.3470e-03,  3.6495e-02],\n",
            "        [-6.2620e-02,  9.4681e-02,  5.6575e-03, -3.3755e-02,  7.3862e-02,\n",
            "          1.0584e-01, -5.8397e-02, -1.6947e-02,  6.1958e-02],\n",
            "        [ 2.5294e-02,  1.2056e-04,  6.7775e-02,  6.1328e-02,  8.2766e-02,\n",
            "          1.5641e-02, -1.8264e-02, -9.1499e-03, -1.6940e-02],\n",
            "        [ 6.6661e-02, -4.2262e-02,  8.7523e-02,  6.7701e-03, -2.0480e-02,\n",
            "          1.9700e-02,  2.4345e-02, -3.5680e-02,  8.6814e-02],\n",
            "        [ 1.0487e-01,  6.3542e-02, -7.4475e-02,  1.3318e-02,  2.9583e-02,\n",
            "          1.1082e-01, -4.3809e-02, -7.3747e-03, -3.4710e-02],\n",
            "        [-8.6959e-03, -3.4587e-02,  5.6932e-02, -1.5701e-02, -1.1438e-02,\n",
            "          8.5409e-02,  4.0407e-02,  4.8809e-02,  5.2773e-02],\n",
            "        [-3.5841e-02,  7.3237e-02,  4.3027e-02,  1.1625e-01, -4.2534e-02,\n",
            "         -6.2440e-02,  1.9526e-02,  1.1460e-02,  5.8530e-03],\n",
            "        [ 5.7185e-02,  1.1741e-02, -3.4775e-02,  8.7757e-02, -2.2177e-02,\n",
            "          1.0279e-02, -5.6578e-02,  8.4985e-02, -2.4100e-02],\n",
            "        [ 7.2821e-02, -2.9298e-02,  2.5951e-02, -1.3231e-02,  3.5417e-02,\n",
            "          1.0153e-01,  3.1472e-02,  5.8870e-02, -8.6941e-02],\n",
            "        [ 3.7947e-02,  6.1065e-02,  6.6134e-02,  4.2039e-02,  1.2015e-02,\n",
            "          7.8713e-02, -7.3039e-02, -1.2827e-02, -3.1060e-03],\n",
            "        [ 1.1044e-01, -3.2046e-02,  4.0199e-02,  5.2018e-02, -4.3138e-02,\n",
            "         -2.9052e-02, -3.7947e-02,  4.7770e-03,  7.8018e-02],\n",
            "        [-3.0444e-02,  1.0447e-01, -2.4959e-02,  5.4660e-02, -2.2353e-02,\n",
            "          1.0089e-01,  4.8524e-02, -6.0509e-03, -4.4797e-02],\n",
            "        [-7.6015e-02,  7.9594e-02, -7.8516e-02,  1.0173e-01,  8.5313e-02,\n",
            "          7.9253e-02, -1.2984e-02, -1.0876e-02,  1.7479e-03],\n",
            "        [-2.9806e-03,  8.7252e-03, -1.5927e-02,  1.4753e-02, -1.1036e-02,\n",
            "          3.6220e-02, -2.7281e-02, -1.1895e-03,  8.7518e-02],\n",
            "        [-1.9067e-02,  1.6909e-02, -3.0582e-02, -6.9658e-02,  3.3916e-02,\n",
            "         -1.5658e-03,  1.0086e-01,  1.0549e-01,  2.4119e-02],\n",
            "        [ 3.0204e-02, -4.7408e-02,  2.2120e-02,  9.0567e-02, -3.8953e-02,\n",
            "         -1.4120e-02,  8.7617e-02,  3.2891e-02,  3.4218e-02],\n",
            "        [ 6.3620e-02,  1.3462e-02, -8.3698e-04, -1.5194e-02,  7.8683e-02,\n",
            "         -2.4856e-02, -4.5221e-02,  5.8156e-02,  3.7225e-02],\n",
            "        [-6.9369e-02, -3.3386e-02,  7.2246e-02, -6.1230e-02,  1.1111e-01,\n",
            "          8.2409e-02,  8.6801e-04,  9.0953e-02, -3.2735e-02],\n",
            "        [ 2.9609e-02, -9.3087e-03,  1.1990e-02,  3.2015e-02,  4.6086e-02,\n",
            "          5.5939e-03,  9.4556e-02,  8.1279e-02, -9.2538e-02],\n",
            "        [ 6.9600e-02, -4.7401e-02,  5.5593e-02, -1.4486e-02, -2.6761e-02,\n",
            "         -3.3408e-02,  6.7409e-03,  2.6640e-02,  8.9794e-02],\n",
            "        [ 9.0104e-02, -1.0700e-02,  1.0751e-02,  5.0200e-02, -6.4520e-02,\n",
            "         -3.2426e-02,  7.4835e-02,  6.8555e-02, -5.3188e-02],\n",
            "        [ 2.0248e-02,  1.0056e-01, -3.7385e-02, -9.3523e-03, -1.9665e-02,\n",
            "          2.6620e-02,  9.9914e-02, -5.0714e-02,  3.5684e-02],\n",
            "        [-3.0659e-02, -6.8398e-03, -1.0621e-02,  4.1601e-02, -2.4201e-02,\n",
            "          3.9465e-02,  9.1007e-02,  1.1518e-01, -6.1028e-02],\n",
            "        [-4.7774e-02,  2.2117e-02, -1.2506e-02, -2.8461e-03, -8.9501e-03,\n",
            "         -3.9934e-03,  9.6354e-02,  7.8627e-02,  2.2492e-02],\n",
            "        [-1.5253e-02,  5.6935e-02,  9.5486e-04,  3.1492e-02,  8.1742e-02,\n",
            "          7.0181e-02, -7.7348e-02,  4.4107e-02, -1.6796e-03],\n",
            "        [ 1.0460e-01,  4.1454e-02, -4.6415e-02,  1.6997e-02, -4.6281e-02,\n",
            "          6.3780e-02, -5.9099e-02,  5.0690e-02,  6.4543e-03],\n",
            "        [-3.4883e-02,  5.7246e-03, -2.8333e-02,  3.1127e-02,  3.4215e-02,\n",
            "          1.3963e-01,  2.2793e-02, -3.8514e-02, -2.4290e-02],\n",
            "        [-1.9772e-02, -6.7915e-02,  3.2497e-03,  6.8910e-02,  7.1238e-03,\n",
            "          1.4009e-02, -1.2327e-02,  3.0470e-02,  1.1739e-01],\n",
            "        [ 5.1326e-02, -8.7911e-03,  8.0207e-02, -4.7809e-02,  9.1827e-02,\n",
            "          7.6584e-02, -5.9921e-02,  5.9024e-02, -7.9322e-02],\n",
            "        [ 3.2346e-02,  6.0513e-02,  6.8418e-02,  9.0830e-02, -1.7767e-02,\n",
            "         -1.1690e-02,  1.6656e-02, -1.0564e-02, -6.8845e-02],\n",
            "        [ 9.1537e-02, -1.0688e-02,  2.1653e-02, -9.0669e-02,  8.0321e-02,\n",
            "          3.2793e-02,  1.0058e-01, -4.6656e-02,  1.8883e-02],\n",
            "        [ 5.6177e-02,  3.2207e-02, -1.7686e-02, -3.5558e-02,  5.1111e-02,\n",
            "          6.7251e-02,  8.3015e-02, -2.0464e-02, -6.2016e-02]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#layer = alexnet_model.features[3].weight.to(device)\n",
        "layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    #print(H_fourier)\n",
        "    #Optimize Hf\n",
        "\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    #H_frob = torch.sum(torch.abs(Hf),dim = 0)\n",
        "    #print(H_frob)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,s_f[0])))\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf \n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.007294982671737671\n",
            "Difference:26.133201599121094\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p='fro')}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SGDMinProject(H0,Ybar,Hbar,rho):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    H = torch.rand_like(H0, requires_grad=True,dtype = torch.complex64).to(device)\n",
        "    optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "    for step in range(500):\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.norm(H - H0, p='fro')**2\n",
        "        loss = loss + (rho) * (torch.norm(H-Hbar-Ybar,p='fro')**2)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #if(step%100 ==0):\n",
        "        #    print(loss)\n",
        "    \n",
        "    return H.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 25])\n",
            "torch.Size([169, 25])\n",
            "torch.Size([12288, 25])\n",
            "----------------Step 0: Primal Residual:21.07489013671875\n",
            "----------------Step 10: Primal Residual:19.143451690673828\n",
            "----------------Step 20: Primal Residual:17.391773223876953\n",
            "----------------Step 30: Primal Residual:15.80850601196289\n",
            "----------------Step 40: Primal Residual:14.377713203430176\n",
            "----------------Step 50: Primal Residual:13.086310386657715\n",
            "----------------Step 60: Primal Residual:11.919387817382812\n",
            "----------------Step 70: Primal Residual:10.866209030151367\n",
            "----------------Step 80: Primal Residual:9.915656089782715\n",
            "----------------Step 90: Primal Residual:9.0564603805542\n",
            "Step 0: Primal Residual:13.667876243591309\n",
            "----------------Step 0: Primal Residual:41.54755401611328\n",
            "----------------Step 10: Primal Residual:37.64556121826172\n",
            "----------------Step 20: Primal Residual:34.11109161376953\n",
            "----------------Step 30: Primal Residual:30.913127899169922\n",
            "----------------Step 40: Primal Residual:28.020057678222656\n",
            "----------------Step 50: Primal Residual:25.40251922607422\n",
            "----------------Step 60: Primal Residual:23.03498649597168\n",
            "----------------Step 70: Primal Residual:20.894330978393555\n",
            "----------------Step 80: Primal Residual:18.958438873291016\n",
            "----------------Step 90: Primal Residual:17.207693099975586\n",
            "----------------Step 0: Primal Residual:50.408973693847656\n",
            "----------------Step 10: Primal Residual:45.66022872924805\n",
            "----------------Step 20: Primal Residual:41.359771728515625\n",
            "----------------Step 30: Primal Residual:37.468101501464844\n",
            "----------------Step 40: Primal Residual:33.94645690917969\n",
            "----------------Step 50: Primal Residual:30.760114669799805\n",
            "----------------Step 60: Primal Residual:27.876850128173828\n",
            "----------------Step 70: Primal Residual:25.269325256347656\n",
            "----------------Step 80: Primal Residual:22.910505294799805\n",
            "----------------Step 90: Primal Residual:20.77653694152832\n",
            "----------------Step 0: Primal Residual:54.050254821777344\n",
            "----------------Step 10: Primal Residual:48.95351028442383\n",
            "----------------Step 20: Primal Residual:44.3381233215332\n",
            "----------------Step 30: Primal Residual:40.161251068115234\n",
            "----------------Step 40: Primal Residual:36.38191604614258\n",
            "----------------Step 50: Primal Residual:32.961944580078125\n",
            "----------------Step 60: Primal Residual:29.867645263671875\n",
            "----------------Step 70: Primal Residual:27.06806755065918\n",
            "----------------Step 80: Primal Residual:24.536113739013672\n",
            "----------------Step 90: Primal Residual:22.246692657470703\n",
            "----------------Step 0: Primal Residual:55.52552032470703\n",
            "----------------Step 10: Primal Residual:50.28744125366211\n",
            "----------------Step 20: Primal Residual:45.54453659057617\n",
            "----------------Step 30: Primal Residual:41.25202941894531\n",
            "----------------Step 40: Primal Residual:37.367698669433594\n",
            "----------------Step 50: Primal Residual:33.852909088134766\n",
            "----------------Step 60: Primal Residual:30.67319107055664\n",
            "----------------Step 70: Primal Residual:27.796422958374023\n",
            "----------------Step 80: Primal Residual:25.194013595581055\n",
            "----------------Step 90: Primal Residual:22.840511322021484\n",
            "----------------Step 0: Primal Residual:56.118526458740234\n",
            "----------------Step 10: Primal Residual:50.82358169555664\n",
            "----------------Step 20: Primal Residual:46.029197692871094\n",
            "----------------Step 30: Primal Residual:41.69014358520508\n",
            "----------------Step 40: Primal Residual:37.763607025146484\n",
            "----------------Step 50: Primal Residual:34.210533142089844\n",
            "----------------Step 60: Primal Residual:30.996185302734375\n",
            "----------------Step 70: Primal Residual:28.088472366333008\n",
            "----------------Step 80: Primal Residual:25.458232879638672\n",
            "----------------Step 90: Primal Residual:23.079484939575195\n",
            "----------------Step 0: Primal Residual:56.355384826660156\n",
            "----------------Step 10: Primal Residual:51.03763961791992\n",
            "----------------Step 20: Primal Residual:46.22255325317383\n",
            "----------------Step 30: Primal Residual:41.86487579345703\n",
            "----------------Step 40: Primal Residual:37.921390533447266\n",
            "----------------Step 50: Primal Residual:34.353118896484375\n",
            "----------------Step 60: Primal Residual:31.124746322631836\n",
            "----------------Step 70: Primal Residual:28.20474624633789\n",
            "----------------Step 80: Primal Residual:25.563446044921875\n",
            "----------------Step 90: Primal Residual:23.174646377563477\n",
            "----------------Step 0: Primal Residual:56.449466705322266\n",
            "----------------Step 10: Primal Residual:51.1225471496582\n",
            "----------------Step 20: Primal Residual:46.29924392700195\n",
            "----------------Step 30: Primal Residual:41.93408203125\n",
            "----------------Step 40: Primal Residual:37.983795166015625\n",
            "----------------Step 50: Primal Residual:34.40962600708008\n",
            "----------------Step 60: Primal Residual:31.175857543945312\n",
            "----------------Step 70: Primal Residual:28.250633239746094\n",
            "----------------Step 80: Primal Residual:25.604961395263672\n",
            "----------------Step 90: Primal Residual:23.21258544921875\n",
            "----------------Step 0: Primal Residual:56.48672103881836\n",
            "----------------Step 10: Primal Residual:51.156158447265625\n",
            "----------------Step 20: Primal Residual:46.32952117919922\n",
            "----------------Step 30: Primal Residual:41.9613037109375\n",
            "----------------Step 40: Primal Residual:38.008445739746094\n",
            "----------------Step 50: Primal Residual:34.43190383911133\n",
            "----------------Step 60: Primal Residual:31.195764541625977\n",
            "----------------Step 70: Primal Residual:28.268640518188477\n",
            "----------------Step 80: Primal Residual:25.62124252319336\n",
            "----------------Step 90: Primal Residual:23.227264404296875\n",
            "----------------Step 0: Primal Residual:56.50138473510742\n",
            "----------------Step 10: Primal Residual:51.16936111450195\n",
            "----------------Step 20: Primal Residual:46.34136962890625\n",
            "----------------Step 30: Primal Residual:41.97203063964844\n",
            "----------------Step 40: Primal Residual:38.01801300048828\n",
            "----------------Step 50: Primal Residual:34.440528869628906\n",
            "----------------Step 60: Primal Residual:31.203414916992188\n",
            "----------------Step 70: Primal Residual:28.275590896606445\n",
            "----------------Step 80: Primal Residual:25.627511978149414\n",
            "----------------Step 90: Primal Residual:23.232982635498047\n",
            "----------------Step 0: Primal Residual:56.50715255737305\n",
            "----------------Step 10: Primal Residual:51.17446517944336\n",
            "----------------Step 20: Primal Residual:46.34602737426758\n",
            "----------------Step 30: Primal Residual:41.97614669799805\n",
            "----------------Step 40: Primal Residual:38.02180099487305\n",
            "----------------Step 50: Primal Residual:34.44377517700195\n",
            "----------------Step 60: Primal Residual:31.20639419555664\n",
            "----------------Step 70: Primal Residual:28.278343200683594\n",
            "----------------Step 80: Primal Residual:25.62993049621582\n",
            "----------------Step 90: Primal Residual:23.235231399536133\n",
            "Step 10: Primal Residual:0.01445421576499939\n",
            "----------------Step 0: Primal Residual:56.509437561035156\n",
            "----------------Step 10: Primal Residual:51.17649459838867\n",
            "----------------Step 20: Primal Residual:46.3477897644043\n",
            "----------------Step 30: Primal Residual:41.9777717590332\n",
            "----------------Step 40: Primal Residual:38.023250579833984\n",
            "----------------Step 50: Primal Residual:34.44501876831055\n",
            "----------------Step 60: Primal Residual:31.207557678222656\n",
            "----------------Step 70: Primal Residual:28.279443740844727\n",
            "----------------Step 80: Primal Residual:25.63079833984375\n",
            "----------------Step 90: Primal Residual:23.236108779907227\n",
            "----------------Step 0: Primal Residual:56.51032638549805\n",
            "----------------Step 10: Primal Residual:51.17729568481445\n",
            "----------------Step 20: Primal Residual:46.34848403930664\n",
            "----------------Step 30: Primal Residual:41.978416442871094\n",
            "----------------Step 40: Primal Residual:38.023807525634766\n",
            "----------------Step 50: Primal Residual:34.44553756713867\n",
            "----------------Step 60: Primal Residual:31.207979202270508\n",
            "----------------Step 70: Primal Residual:28.279874801635742\n",
            "----------------Step 80: Primal Residual:25.631153106689453\n",
            "----------------Step 90: Primal Residual:23.23643684387207\n",
            "----------------Step 0: Primal Residual:56.510643005371094\n",
            "----------------Step 10: Primal Residual:51.1776123046875\n",
            "----------------Step 20: Primal Residual:46.34880065917969\n",
            "----------------Step 30: Primal Residual:41.978668212890625\n",
            "----------------Step 40: Primal Residual:38.02404022216797\n",
            "----------------Step 50: Primal Residual:34.44572067260742\n",
            "----------------Step 60: Primal Residual:31.20813751220703\n",
            "----------------Step 70: Primal Residual:28.28004264831543\n",
            "----------------Step 80: Primal Residual:25.631296157836914\n",
            "----------------Step 90: Primal Residual:23.236562728881836\n",
            "----------------Step 0: Primal Residual:56.51079177856445\n",
            "----------------Step 10: Primal Residual:51.17774963378906\n",
            "----------------Step 20: Primal Residual:46.34888458251953\n",
            "----------------Step 30: Primal Residual:41.978763580322266\n",
            "----------------Step 40: Primal Residual:38.02411651611328\n",
            "----------------Step 50: Primal Residual:34.44579315185547\n",
            "----------------Step 60: Primal Residual:31.208189010620117\n",
            "----------------Step 70: Primal Residual:28.28009796142578\n",
            "----------------Step 80: Primal Residual:25.6313533782959\n",
            "----------------Step 90: Primal Residual:23.23661231994629\n",
            "----------------Step 0: Primal Residual:56.51089096069336\n",
            "----------------Step 10: Primal Residual:51.177825927734375\n",
            "----------------Step 20: Primal Residual:46.348976135253906\n",
            "----------------Step 30: Primal Residual:41.97878646850586\n",
            "----------------Step 40: Primal Residual:38.02415084838867\n",
            "----------------Step 50: Primal Residual:34.44581604003906\n",
            "----------------Step 60: Primal Residual:31.20821189880371\n",
            "----------------Step 70: Primal Residual:28.280113220214844\n",
            "----------------Step 80: Primal Residual:25.63138198852539\n",
            "----------------Step 90: Primal Residual:23.236631393432617\n",
            "----------------Step 0: Primal Residual:56.51090621948242\n",
            "----------------Step 10: Primal Residual:51.17783737182617\n",
            "----------------Step 20: Primal Residual:46.348960876464844\n",
            "----------------Step 30: Primal Residual:41.97880935668945\n",
            "----------------Step 40: Primal Residual:38.02416229248047\n",
            "----------------Step 50: Primal Residual:34.44581985473633\n",
            "----------------Step 60: Primal Residual:31.208215713500977\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63139533996582\n",
            "----------------Step 90: Primal Residual:23.23664093017578\n",
            "----------------Step 0: Primal Residual:56.51097869873047\n",
            "----------------Step 10: Primal Residual:51.17784881591797\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.978824615478516\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44581604003906\n",
            "----------------Step 60: Primal Residual:31.208221435546875\n",
            "----------------Step 70: Primal Residual:28.280118942260742\n",
            "----------------Step 80: Primal Residual:25.631399154663086\n",
            "----------------Step 90: Primal Residual:23.236642837524414\n",
            "----------------Step 0: Primal Residual:56.51095199584961\n",
            "----------------Step 10: Primal Residual:51.17780685424805\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208223342895508\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.631406784057617\n",
            "----------------Step 90: Primal Residual:23.23664665222168\n",
            "----------------Step 0: Primal Residual:56.51097106933594\n",
            "----------------Step 10: Primal Residual:51.177879333496094\n",
            "----------------Step 20: Primal Residual:46.34895324707031\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.4458122253418\n",
            "----------------Step 60: Primal Residual:31.208221435546875\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.631406784057617\n",
            "----------------Step 90: Primal Residual:23.236650466918945\n",
            "----------------Step 0: Primal Residual:56.51090621948242\n",
            "----------------Step 10: Primal Residual:51.17787551879883\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44581985473633\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.236650466918945\n",
            "Step 20: Primal Residual:0.00013614300405606627\n",
            "----------------Step 0: Primal Residual:56.51100158691406\n",
            "----------------Step 10: Primal Residual:51.17787551879883\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883224487305\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.236650466918945\n",
            "----------------Step 0: Primal Residual:56.510963439941406\n",
            "----------------Step 10: Primal Residual:51.17783737182617\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208223342895508\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.51095962524414\n",
            "----------------Step 10: Primal Residual:51.17790222167969\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.97882080078125\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280120849609375\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51091003417969\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.3490104675293\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51102828979492\n",
            "----------------Step 10: Primal Residual:51.17787551879883\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.978816986083984\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51094436645508\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.511009216308594\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.17784118652344\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44581985473633\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510982513427734\n",
            "----------------Step 10: Primal Residual:51.17787551879883\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.0241584777832\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 30: Primal Residual:3.954651765525341e-05\n",
            "----------------Step 0: Primal Residual:56.510986328125\n",
            "----------------Step 10: Primal Residual:51.177894592285156\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.17784881591797\n",
            "----------------Step 20: Primal Residual:46.348968505859375\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280120849609375\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51093292236328\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510921478271484\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.3490104675293\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097869873047\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.51093673706055\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.349021911621094\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.02415466308594\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51100158691406\n",
            "----------------Step 10: Primal Residual:51.17789077758789\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.51100540161133\n",
            "----------------Step 10: Primal Residual:51.17787170410156\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.02416229248047\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51094055175781\n",
            "----------------Step 10: Primal Residual:51.17790222167969\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97882080078125\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208223342895508\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51103591918945\n",
            "----------------Step 10: Primal Residual:51.1778450012207\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.978824615478516\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "Step 40: Primal Residual:4.376040669740178e-05\n",
            "----------------Step 0: Primal Residual:56.51093292236328\n",
            "----------------Step 10: Primal Residual:51.17782211303711\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51096725463867\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510955810546875\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883224487305\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.177894592285156\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51095962524414\n",
            "----------------Step 10: Primal Residual:51.1778450012207\n",
            "----------------Step 20: Primal Residual:46.34899139404297\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510921478271484\n",
            "----------------Step 10: Primal Residual:51.177825927734375\n",
            "----------------Step 20: Primal Residual:46.3489990234375\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.17787551879883\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.208221435546875\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.510955810546875\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.3490104675293\n",
            "----------------Step 30: Primal Residual:41.978816986083984\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208223342895508\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51096725463867\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.978851318359375\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097869873047\n",
            "----------------Step 10: Primal Residual:51.17789840698242\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 50: Primal Residual:7.352246029768139e-05\n",
            "----------------Step 0: Primal Residual:56.51091766357422\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.34896469116211\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.510990142822266\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511016845703125\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511009216308594\n",
            "----------------Step 10: Primal Residual:51.17787170410156\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97882080078125\n",
            "----------------Step 40: Primal Residual:38.02416229248047\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51087188720703\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097869873047\n",
            "----------------Step 10: Primal Residual:51.17784881591797\n",
            "----------------Step 20: Primal Residual:46.348960876464844\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.17784881591797\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.97883224487305\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.511016845703125\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.348960876464844\n",
            "----------------Step 30: Primal Residual:41.978824615478516\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511016845703125\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.34895706176758\n",
            "----------------Step 30: Primal Residual:41.978816986083984\n",
            "----------------Step 40: Primal Residual:38.02415084838867\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510990142822266\n",
            "----------------Step 10: Primal Residual:51.177852630615234\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 60: Primal Residual:6.017782288836315e-05\n",
            "----------------Step 0: Primal Residual:56.5109748840332\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44581985473633\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.5109977722168\n",
            "----------------Step 10: Primal Residual:51.177825927734375\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.0241584777832\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097106933594\n",
            "----------------Step 10: Primal Residual:51.1778450012207\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51093292236328\n",
            "----------------Step 10: Primal Residual:51.17781066894531\n",
            "----------------Step 20: Primal Residual:46.34902572631836\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51095199584961\n",
            "----------------Step 10: Primal Residual:51.17784118652344\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511016845703125\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.236652374267578\n",
            "----------------Step 0: Primal Residual:56.5109748840332\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.348968505859375\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44581985473633\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510982513427734\n",
            "----------------Step 10: Primal Residual:51.177879333496094\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51092529296875\n",
            "----------------Step 10: Primal Residual:51.1778450012207\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.236656188964844\n",
            "----------------Step 0: Primal Residual:56.51103973388672\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.978816986083984\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280120849609375\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 70: Primal Residual:6.131974078016356e-05\n",
            "----------------Step 0: Primal Residual:56.510948181152344\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.02415466308594\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.236656188964844\n",
            "----------------Step 0: Primal Residual:56.511024475097656\n",
            "----------------Step 10: Primal Residual:51.17789840698242\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511024475097656\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.348976135253906\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20821762084961\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097106933594\n",
            "----------------Step 10: Primal Residual:51.177852630615234\n",
            "----------------Step 20: Primal Residual:46.349021911621094\n",
            "----------------Step 30: Primal Residual:41.97882080078125\n",
            "----------------Step 40: Primal Residual:38.024169921875\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51088333129883\n",
            "----------------Step 10: Primal Residual:51.17789077758789\n",
            "----------------Step 20: Primal Residual:46.348995208740234\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.02415466308594\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51096725463867\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510990142822266\n",
            "----------------Step 10: Primal Residual:51.17781066894531\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.5108528137207\n",
            "----------------Step 10: Primal Residual:51.17790222167969\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208223342895508\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.5109748840332\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.280122756958008\n",
            "----------------Step 80: Primal Residual:25.631410598754883\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51100540161133\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883987426758\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280120849609375\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 80: Primal Residual:8.832270395942032e-05\n",
            "----------------Step 0: Primal Residual:56.51097106933594\n",
            "----------------Step 10: Primal Residual:51.17788314819336\n",
            "----------------Step 20: Primal Residual:46.3489875793457\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510902404785156\n",
            "----------------Step 10: Primal Residual:51.1778678894043\n",
            "----------------Step 20: Primal Residual:46.34901809692383\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51101303100586\n",
            "----------------Step 10: Primal Residual:51.177894592285156\n",
            "----------------Step 20: Primal Residual:46.34899139404297\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.02416229248047\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.5109748840332\n",
            "----------------Step 10: Primal Residual:51.17782974243164\n",
            "----------------Step 20: Primal Residual:46.34897994995117\n",
            "----------------Step 30: Primal Residual:41.978851318359375\n",
            "----------------Step 40: Primal Residual:38.0241813659668\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20822525024414\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51095199584961\n",
            "----------------Step 10: Primal Residual:51.17783737182617\n",
            "----------------Step 20: Primal Residual:46.349021911621094\n",
            "----------------Step 30: Primal Residual:41.97881317138672\n",
            "----------------Step 40: Primal Residual:38.02416229248047\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510921478271484\n",
            "----------------Step 10: Primal Residual:51.17783737182617\n",
            "----------------Step 20: Primal Residual:46.3490104675293\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510955810546875\n",
            "----------------Step 10: Primal Residual:51.17787170410156\n",
            "----------------Step 20: Primal Residual:46.349021911621094\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024166107177734\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.63141632080078\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.510986328125\n",
            "----------------Step 10: Primal Residual:51.17789840698242\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97882843017578\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.4458122253418\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.280128479003906\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51102066040039\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.20823097229004\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.511016845703125\n",
            "----------------Step 10: Primal Residual:51.177764892578125\n",
            "----------------Step 20: Primal Residual:46.34901428222656\n",
            "----------------Step 30: Primal Residual:41.97882080078125\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.631412506103516\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "Step 90: Primal Residual:6.879206921439618e-05\n",
            "----------------Step 0: Primal Residual:56.51095962524414\n",
            "----------------Step 10: Primal Residual:51.177860260009766\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.97884750366211\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.44582748413086\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51089096069336\n",
            "----------------Step 10: Primal Residual:51.17786407470703\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n",
            "----------------Step 30: Primal Residual:41.97880554199219\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445831298828125\n",
            "----------------Step 60: Primal Residual:31.208227157592773\n",
            "----------------Step 70: Primal Residual:28.280126571655273\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51097106933594\n",
            "----------------Step 10: Primal Residual:51.17782974243164\n",
            "----------------Step 20: Primal Residual:46.349021911621094\n",
            "----------------Step 30: Primal Residual:41.978843688964844\n",
            "----------------Step 40: Primal Residual:38.024173736572266\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.5109977722168\n",
            "----------------Step 10: Primal Residual:51.17784118652344\n",
            "----------------Step 20: Primal Residual:46.349002838134766\n",
            "----------------Step 30: Primal Residual:41.97883605957031\n",
            "----------------Step 40: Primal Residual:38.02417755126953\n",
            "----------------Step 50: Primal Residual:34.445823669433594\n",
            "----------------Step 60: Primal Residual:31.208229064941406\n",
            "----------------Step 70: Primal Residual:28.28012466430664\n",
            "----------------Step 80: Primal Residual:25.63141441345215\n",
            "----------------Step 90: Primal Residual:23.23665428161621\n",
            "----------------Step 0: Primal Residual:56.51099395751953\n",
            "----------------Step 10: Primal Residual:51.177886962890625\n",
            "----------------Step 20: Primal Residual:46.34900665283203\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m Hf_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(torch\u001b[38;5;241m.\u001b[39mpermute(Hf_2,(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)),(s[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39ms[\u001b[38;5;241m1\u001b[39m],n\u001b[38;5;241m*\u001b[39mn))\n\u001b[1;32m     67\u001b[0m H_bar \u001b[38;5;241m=\u001b[39m (Hf_1\u001b[38;5;241m+\u001b[39mHf_2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 68\u001b[0m Hf_z \u001b[38;5;241m=\u001b[39m \u001b[43mSGDMinProject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHf_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43mH_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m Y1 \u001b[38;5;241m=\u001b[39m Y1 \u001b[38;5;241m+\u001b[39m (Hf_1 \u001b[38;5;241m-\u001b[39m Hf_z)\n\u001b[1;32m     71\u001b[0m Y2 \u001b[38;5;241m=\u001b[39m Y2 \u001b[38;5;241m+\u001b[39m (Hf_2 \u001b[38;5;241m-\u001b[39m Hf_z)\n",
            "Cell \u001b[0;32mIn[49], line 8\u001b[0m, in \u001b[0;36mSGDMinProject\u001b[0;34m(H0, Ybar, Hbar, rho)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mH0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m (rho) \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnorm(H\u001b[38;5;241m-\u001b[39mHbar\u001b[38;5;241m-\u001b[39mYbar,p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    #print(H_fourier)\n",
        "    #Optimize Hf\n",
        "\n",
        "    Hf_0 = H_fourier + U\n",
        "    Y1 = torch.zeros(Hf_0.shape,dtype=torch.complex64,device=device)\n",
        "    Y2 = torch.zeros(Hf_0.shape,dtype=torch.complex64,device=device)\n",
        "    \n",
        "    Hf_1 = Hf_0\n",
        "    Hf_2 = Hf_0\n",
        "    Hf_z = Hf_0\n",
        "    H_bar = (Hf_1+Hf_2)/2\n",
        "    Y_bar = (Y1 + Y2)/2\n",
        "\n",
        "    for j in range(100):\n",
        "        X1 = Hf_z - Y1\n",
        "        X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "        X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=2),min=1.0)\n",
        "        Hf_1 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,1,s[1]))))\n",
        "        Hf_1 = torch.reshape(torch.permute(Hf_1,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "        X1 = Hf_z - Y2\n",
        "        X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "        X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=3),min=1.0)\n",
        "        Hf_2 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,s[0],1))))\n",
        "        Hf_2 = torch.reshape(torch.permute(Hf_2,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "        H_bar = (Hf_1+Hf_2)/2\n",
        "        Hf_z = SGDMinProject(Hf_0,Y_bar,H_bar,0.01)\n",
        "        \n",
        "        Y1 = Y1 + (Hf_1 - Hf_z)\n",
        "        Y2 = Y2 + (Hf_2 - Hf_z)\n",
        "        Y_bar = (Y1 + Y2)/2\n",
        "        if(j%10 == 0):\n",
        "            print(f'----------------Step {j}: Primal Residual:{torch.norm(Hf_z - H_bar,p='fro')}')\n",
        "        \n",
        "    Hf = Hf_z\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf \n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 1.437525987625122\n",
            "Difference:23.297317504882812\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p='fro')}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AA(Anderson_map,s):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    length = len(Anderson_map)\n",
        "\n",
        "    theta = torch.zeros(length-1,device=device, requires_grad=True)\n",
        "    optimizer = torch.optim.Adam([theta], lr=0.01)\n",
        "\n",
        "    for step in range(100):\n",
        "        optimizer.zero_grad()\n",
        "        sum_all = torch.zeros(s,requires_grad=True,device=device)\n",
        "        \n",
        "        for i in range(length-1):\n",
        "            _,F=Anderson_map[i]\n",
        "            _,F1=Anderson_map[i+1]\n",
        "            sum_all = sum_all + theta[i]*(F1-F)\n",
        "        \n",
        "        _,F = Anderson_map[length-1]\n",
        "        loss = torch.norm(F - sum_all,p='fro')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    ans,_ = Anderson_map[length-1]\n",
        "    for i in range(length-1):\n",
        "        G,_=Anderson_map[i]\n",
        "        G1,_=Anderson_map[i+1]\n",
        "        ans = ans - theta[i]*(G1-G)\n",
        "    \n",
        "    return ans.detach()\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 25])\n",
            "torch.Size([169, 25])\n",
            "----------------Step 0: Primal Residual:21.07489013671875\n",
            "----------------Step 10: Primal Residual:19.143449783325195\n",
            "----------------Step 20: Primal Residual:17.391775131225586\n",
            "----------------Step 30: Primal Residual:15.80850601196289\n",
            "----------------Step 40: Primal Residual:14.377713203430176\n",
            "----------------Step 50: Primal Residual:13.086310386657715\n",
            "----------------Step 60: Primal Residual:11.919387817382812\n",
            "----------------Step 70: Primal Residual:10.866209030151367\n",
            "----------------Step 80: Primal Residual:9.915656089782715\n",
            "----------------Step 90: Primal Residual:9.0564603805542\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[58], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m>\u001b[39m(m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    100\u001b[0m         Anderson_map\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m     Hf,U \u001b[38;5;241m=\u001b[39m AA(Anderson_map,(s[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39ms[\u001b[38;5;241m1\u001b[39m],n\u001b[38;5;241m*\u001b[39mn\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     Hf \u001b[38;5;241m=\u001b[39m Hf_def\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize - Anderson Accelerate\n",
        "import torch\n",
        "\n",
        "Anderson_map = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "H_def = H\n",
        "U_def = U\n",
        "Hf_def = Hf\n",
        "\n",
        "res_prev = 1000000\n",
        "reset=True\n",
        "rho = 0.1\n",
        "m = 3\n",
        "iter = 0\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(10):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    #print(H_fourier)\n",
        "    #Optimize Hf\n",
        "\n",
        "    Hf_0 = H_fourier + U\n",
        "    Y1 = torch.zeros(Hf_0.shape,dtype=torch.complex64,device=device)\n",
        "    Y2 = torch.zeros(Hf_0.shape,dtype=torch.complex64,device=device)\n",
        "    \n",
        "    Hf_1 = Hf_0\n",
        "    Hf_2 = Hf_0\n",
        "    Hf_z = Hf_0\n",
        "    H_bar = (Hf_1+Hf_2)/2\n",
        "    Y_bar = (Y1 + Y2)/2\n",
        "    \n",
        "    for j in range(100):\n",
        "        X1 = Hf_z - Y1\n",
        "        X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "        X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=2),min=1.0)\n",
        "        Hf_1 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,1,s[1]))))\n",
        "        Hf_1 = torch.reshape(torch.permute(Hf_1,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "        X1 = Hf_z - Y2\n",
        "        X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "        X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=3),min=1.0)\n",
        "        Hf_2 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,s[0],1))))\n",
        "        Hf_2 = torch.reshape(torch.permute(Hf_2,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "        H_bar = (Hf_1+Hf_2)/2\n",
        "        Hf_z = SGDMinProject(Hf_0,Y_bar,H_bar,0.01)\n",
        "        \n",
        "        Y1 = Y1 + (Hf_1 - Hf_z)\n",
        "        Y2 = Y2 + (Hf_2 - Hf_z)\n",
        "        Y_bar = (Y1 + Y2)/2\n",
        "        if(j%10 == 0):\n",
        "            print(f'----------------Step {j}: Primal Residual:{torch.norm(Hf_z - H_bar,p='fro')}')\n",
        "\n",
        "    dual_res = torch.norm(Hf-Hf_z,p='fro')**2    \n",
        "    Hf = Hf_z\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf \n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')**2\n",
        "    #i=i+1\n",
        "\n",
        "    res = pri_res+dual_res\n",
        "    if((reset==True) or (res<res_prev)):\n",
        "        H_def = H\n",
        "        Hf_def = Hf\n",
        "        U_def = U\n",
        "        res_prev = res\n",
        "        reset = False\n",
        "        iter = iter+1\n",
        "        Anderson_map.append((torch.hstack((Hf,U)),torch.hstack((Hf-Hf_z,H_fourier-Hf))))\n",
        "        if(iter>(m+1)):\n",
        "            Anderson_map.pop(0)\n",
        "\n",
        "        G = AA(Anderson_map,(s[0]*s[1],n*n*2))\n",
        "        Hf = G[:,n*n]\n",
        "        U = G[:,n*n]\n",
        "    else:\n",
        "        Hf = Hf_def\n",
        "        U = U_def\n",
        "        reset = True\n",
        "    \n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([98304, 9])\n",
            "torch.Size([169, 9])\n",
            "Step 0: Primal Residual:23.97040557861328\n",
            "Step 10: Primal Residual:0.5088467597961426\n",
            "Step 20: Primal Residual:0.21946126222610474\n",
            "Step 30: Primal Residual:0.13900388777256012\n",
            "Step 40: Primal Residual:0.10494109243154526\n",
            "Step 50: Primal Residual:0.08867479860782623\n",
            "Step 60: Primal Residual:0.08029833436012268\n",
            "Step 70: Primal Residual:0.07572028785943985\n",
            "Step 80: Primal Residual:0.07296615093946457\n",
            "Step 90: Primal Residual:0.07111893594264984\n",
            "Optimized H:\n",
            "tensor([[ 3.0407e-04, -2.0102e-04, -5.6735e-04,  ..., -2.1678e-03,\n",
            "          8.2489e-04, -1.0365e-03],\n",
            "        [ 1.5948e-03, -1.5966e-04,  1.1119e-03,  ...,  1.1945e-04,\n",
            "          5.3272e-04,  6.6553e-04],\n",
            "        [-1.8734e-03, -1.2673e-03,  9.8569e-04,  ..., -1.5655e-03,\n",
            "         -1.7978e-03,  1.9610e-03],\n",
            "        ...,\n",
            "        [-7.5574e-05,  1.4317e-03,  1.8691e-03,  ...,  3.2356e-04,\n",
            "          1.6334e-03, -4.0907e-04],\n",
            "        [-1.4631e-03, -5.4712e-04,  1.3728e-03,  ..., -1.4042e-03,\n",
            "          2.8615e-04,  1.6764e-03],\n",
            "        [-5.1505e-04, -1.0496e-03, -6.4191e-04,  ..., -2.4025e-04,\n",
            "          2.6383e-03, -3.0212e-04]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM L1-LInfty - WORKING !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[8].weight.to(device)\n",
        "layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "H_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    X1 =  Hf - U1\n",
        "    X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "    X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=2),min=1.0)\n",
        "    Hf1 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,1,s[1]))))\n",
        "    Hf1 = torch.reshape(torch.permute(Hf1,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "    X1 = Hf - U2\n",
        "    X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "    X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=3),min=1.0)\n",
        "    Hf2 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,s[0],1))))\n",
        "    Hf2 = torch.reshape(torch.permute(Hf2,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    H_bar = (H_fourier + Hf1 + Hf2)/3\n",
        "    Hf = H_bar + U_bar\n",
        "    \n",
        "    U = U + H_fourier - Hf\n",
        "    U1 = U1 + Hf1 - Hf\n",
        "    U2 = U2 + Hf2 - Hf\n",
        "    U_bar = (U1+U2+U)/3\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    \n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.3220367133617401\n",
            "Difference: 25.249536514282227\n",
            "Original H:\n",
            "tensor([[-0.0020, -0.0081, -0.0114,  ..., -0.0541, -0.0012, -0.0244],\n",
            "        [ 0.0350,  0.0133,  0.0260,  ...,  0.0035,  0.0181,  0.0147],\n",
            "        [-0.0572, -0.0474,  0.0019,  ..., -0.0515, -0.0490,  0.0254],\n",
            "        ...,\n",
            "        [ 0.0133,  0.0259,  0.0499,  ...,  0.0153,  0.0184,  0.0001],\n",
            "        [-0.0324, -0.0163,  0.0237,  ..., -0.0236,  0.0041,  0.0363],\n",
            "        [-0.0293, -0.0176, -0.0268,  ...,  0.0003,  0.0701,  0.0068]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference: {torch.norm((H - H0), p=\"fro\")}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cvxpy as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ADMM L1-LInfty - CVXPY\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 13\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "H_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    X1 =  Hf - U1\n",
        "    X2 = Hf - U2\n",
        "    for j in range(n*n):\n",
        "        c1 = torch.reshape(X1[:,j],(s[0],s[1]))\n",
        "        c1_proj = cp.Variable((s[0],s[1]),complex=True)\n",
        "        objective = cp.Minimize(cp.norm(c1_proj - (c1.cpu()).numpy(),'fro'))\n",
        "        constraints = [cp.norm(c1_proj,1)<=1]\n",
        "        prob = cp.Problem(objective,constraints)\n",
        "        prob.solve(solver=cp.SCS)\n",
        "        c1 = c1_proj.value()\n",
        "        X1[:,j] = torch.reshape(torch.from_numpy(c1),s[0]*s[1])\n",
        "\n",
        "        c2 = torch.reshape(X2[:,j],(s[0],s[1]))\n",
        "        c2_proj = cp.Variable((s[0],s[1]),complex=True)\n",
        "        objective = cp.Minimize(cp.norm(c2_proj - (c2.cpu()).numpy(),'fro'))\n",
        "        constraints = [cp.norm(c2_proj,1)<=1]\n",
        "        prob = cp.Problem(objective,constraints)\n",
        "        prob.solve(solver=cp.SCS)\n",
        "        c2 = c2_proj.value()\n",
        "        X2[:,j] = torch.reshape(torch.from_numpy(c2),s[0]*s[1])\n",
        "\n",
        "    Hf1 = X1\n",
        "    Hf2 = X2\n",
        "                        \n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    H_bar = (H_fourier + Hf1 + Hf2)/3\n",
        "    Hf = H_bar + U_bar\n",
        "    \n",
        "    U = U + H_fourier - Hf\n",
        "    U1 = U1 + Hf1 - Hf\n",
        "    U2 = U2 + Hf2 - Hf\n",
        "    U_bar = (U1+U2+U)/3\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    \n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([36, 9])\n",
            "torch.Size([1600, 9])\n",
            "Step 0: Primal Residual:16.55470085144043,Dual Residual:2.7591171264648438\n",
            "Step 10: Primal Residual:0.7833991050720215,Dual Residual:1.9197267293930054\n",
            "Step 20: Primal Residual:0.6151298880577087,Dual Residual:0.7685045003890991\n",
            "Step 30: Primal Residual:0.08681049942970276,Dual Residual:0.49097153544425964\n",
            "Step 40: Primal Residual:0.08625029027462006,Dual Residual:0.39104995131492615\n",
            "Step 50: Primal Residual:0.3090822398662567,Dual Residual:0.1815582811832428\n",
            "Step 60: Primal Residual:0.16735610365867615,Dual Residual:0.1204300969839096\n",
            "Step 70: Primal Residual:0.11745654046535492,Dual Residual:0.06231284886598587\n",
            "Step 80: Primal Residual:0.06214126944541931,Dual Residual:0.043409936130046844\n",
            "Step 90: Primal Residual:0.04422123730182648,Dual Residual:0.02841004729270935\n",
            "Step 100: Primal Residual:0.030799565836787224,Dual Residual:0.019342172890901566\n",
            "Step 110: Primal Residual:0.023048851639032364,Dual Residual:0.012206587009131908\n",
            "Step 120: Primal Residual:0.017867304384708405,Dual Residual:0.008383977226912975\n",
            "Step 130: Primal Residual:0.013182437978684902,Dual Residual:0.006094565615057945\n",
            "Step 140: Primal Residual:0.009913263842463493,Dual Residual:0.004611543379724026\n",
            "Step 150: Primal Residual:0.008663696236908436,Dual Residual:0.003254476934671402\n",
            "Step 160: Primal Residual:0.0068749901838600636,Dual Residual:0.0026433903258293867\n",
            "Step 170: Primal Residual:0.005702141206711531,Dual Residual:0.0021015771199017763\n",
            "Step 180: Primal Residual:0.004893322009593248,Dual Residual:0.001568820676766336\n",
            "Step 190: Primal Residual:0.003946393728256226,Dual Residual:0.0012517210561782122\n",
            "Optimized H:\n",
            "tensor([[ 8.0967e-02, -3.9691e-02, -2.4656e-02, -4.2886e-02,  2.6452e-02,\n",
            "         -1.1815e-02,  2.5432e-02,  4.7359e-02,  1.1869e-01],\n",
            "        [-6.7932e-03, -7.9214e-03,  6.8536e-02,  1.3706e-01, -7.4543e-02,\n",
            "         -1.7156e-03,  1.0593e-01, -3.5271e-02, -7.5732e-03],\n",
            "        [-1.1247e-02,  8.9695e-02,  1.6280e-02, -1.1052e-02, -1.8451e-02,\n",
            "         -5.5765e-02,  7.4600e-02, -2.9344e-02,  8.6088e-02],\n",
            "        [-2.1650e-02,  1.0514e-01, -1.9464e-02,  1.1539e-02,  1.5604e-02,\n",
            "         -2.9575e-02,  2.5476e-02,  1.0204e-01,  3.4445e-03],\n",
            "        [-4.7693e-02,  2.6841e-02,  5.2625e-02,  6.1758e-02,  5.0762e-02,\n",
            "         -1.8797e-02,  5.1447e-02, -3.6331e-02, -1.6489e-02],\n",
            "        [-7.3966e-02,  1.0154e-02, -2.5302e-02,  1.0433e-01, -5.3972e-02,\n",
            "         -6.9003e-03,  4.1974e-02,  1.1171e-01,  7.6465e-02],\n",
            "        [ 1.4363e-02, -1.0700e-02, -3.4348e-02,  5.3702e-02, -2.1432e-02,\n",
            "          9.5966e-02,  3.7552e-02, -3.7746e-02,  7.5015e-02],\n",
            "        [ 6.5609e-02,  2.5936e-02,  1.1169e-01, -6.8841e-02,  6.4229e-02,\n",
            "          5.3179e-03,  5.4689e-02, -2.9160e-02, -6.4537e-02],\n",
            "        [ 3.9724e-02,  3.9149e-02,  6.6246e-02,  6.3395e-02,  3.6599e-02,\n",
            "          2.8172e-02, -4.7901e-02, -9.3011e-02,  6.9941e-02],\n",
            "        [-6.5205e-02,  7.4791e-02,  6.9301e-02,  3.7190e-02, -9.6456e-02,\n",
            "          9.3556e-02, -4.9239e-03,  3.2921e-02,  1.0394e-02],\n",
            "        [ 1.2080e-01,  1.0079e-01, -3.1631e-02, -3.4713e-02, -5.5034e-03,\n",
            "         -5.4237e-02,  3.6882e-02,  5.3847e-03,  2.7514e-02],\n",
            "        [ 4.3594e-03,  4.1929e-03, -6.0896e-03, -1.8896e-02,  7.4913e-02,\n",
            "          3.8559e-02,  2.9186e-02,  8.0716e-02, -6.1602e-02],\n",
            "        [ 4.2220e-02,  8.2309e-03,  1.8858e-02, -3.5129e-02,  1.6110e-01,\n",
            "          1.7351e-02, -2.8262e-02, -1.7649e-02, -4.6859e-02],\n",
            "        [ 1.4099e-01, -9.7653e-03, -5.5528e-02,  4.7405e-02,  1.9152e-02,\n",
            "         -1.8925e-02,  2.4798e-02, -6.1860e-03,  2.2644e-03],\n",
            "        [ 7.4750e-02, -5.8273e-05,  4.4108e-02, -3.3684e-02,  8.2841e-02,\n",
            "          4.6688e-03, -5.5860e-02,  1.8023e-02,  7.1477e-02],\n",
            "        [-5.0959e-02, -3.1787e-02, -1.7301e-02, -1.2893e-02,  9.5162e-02,\n",
            "          6.8738e-02,  3.4678e-02,  8.2467e-02, -1.7636e-02],\n",
            "        [ 8.2856e-02, -7.6962e-02,  6.7530e-02,  2.1335e-02,  8.1937e-02,\n",
            "          7.0162e-02,  4.5224e-02, -5.9631e-02, -1.6340e-02],\n",
            "        [ 5.1292e-02, -1.0811e-02,  3.7217e-02, -3.4842e-02,  1.2713e-03,\n",
            "         -1.5252e-02,  6.8158e-02,  9.7736e-02, -4.3146e-02],\n",
            "        [-2.1532e-02, -8.7460e-03, -2.1630e-02,  1.5772e-02, -1.5130e-02,\n",
            "          6.0736e-02,  6.8826e-02,  1.1032e-01,  4.6023e-03],\n",
            "        [ 3.3395e-02, -2.1833e-02, -2.1864e-02, -6.2043e-02,  7.7435e-02,\n",
            "         -4.8677e-02,  4.5422e-02,  1.1112e-01,  4.1061e-02],\n",
            "        [-2.7761e-02,  1.2258e-01, -1.9978e-02, -4.0509e-02, -7.2701e-03,\n",
            "          1.2717e-02,  2.6541e-02,  7.0106e-02, -2.7528e-02],\n",
            "        [ 5.8508e-02, -1.1744e-02,  1.9756e-03,  3.6129e-02, -7.5018e-02,\n",
            "         -7.8203e-03,  3.2107e-02,  1.1669e-01, -7.0738e-03],\n",
            "        [-2.4555e-03,  8.8234e-03, -3.4613e-02,  4.0413e-02,  6.7260e-03,\n",
            "          9.6887e-02, -6.7005e-03, -5.4521e-02,  1.2699e-01],\n",
            "        [ 4.2262e-02,  9.9699e-02,  2.6568e-02,  7.3920e-02,  1.8888e-02,\n",
            "         -3.6383e-02, -1.6707e-02,  5.2383e-02, -4.1880e-02],\n",
            "        [ 1.7462e-02, -5.8565e-02,  6.0035e-02, -3.5367e-02,  6.3342e-02,\n",
            "          1.1119e-02,  7.3817e-02,  1.2445e-02, -8.4501e-03],\n",
            "        [ 1.1043e-01,  7.8154e-02, -2.8200e-02, -3.0902e-02, -7.4467e-02,\n",
            "          2.0621e-02,  5.8688e-02,  1.8789e-02,  1.8967e-02],\n",
            "        [-8.5115e-02,  1.5943e-02,  8.3514e-02, -4.9097e-02,  6.0527e-03,\n",
            "          5.9823e-02,  6.1172e-02, -3.1976e-02,  1.0388e-01],\n",
            "        [ 3.5673e-02, -9.3573e-02,  1.5921e-02,  9.7117e-02,  4.1993e-02,\n",
            "          4.4493e-02,  1.7701e-02,  6.4802e-02,  1.4179e-02],\n",
            "        [ 7.0899e-02,  7.4443e-02, -6.8586e-02, -5.8160e-03,  1.2951e-01,\n",
            "          9.7304e-03,  3.0232e-02, -5.2144e-02, -2.2407e-02],\n",
            "        [ 4.2835e-02, -1.9737e-02, -6.9604e-02,  5.9024e-02, -3.5081e-02,\n",
            "          4.2819e-02,  1.6912e-02,  2.4837e-03,  8.3276e-02],\n",
            "        [ 3.3224e-02,  3.3449e-02,  8.8414e-02, -4.7643e-02, -8.7045e-02,\n",
            "          7.3642e-02,  1.0214e-01,  1.7482e-02, -8.3734e-03],\n",
            "        [-1.0998e-02,  5.1639e-02, -1.1523e-02,  7.6158e-03,  3.0441e-02,\n",
            "         -9.9378e-03,  5.3123e-02,  1.1579e-02,  6.8291e-02],\n",
            "        [-2.7572e-02, -1.1551e-03, -1.3168e-02,  2.5280e-02,  1.0777e-01,\n",
            "          5.0298e-02,  8.7050e-02, -2.6567e-02, -2.6833e-02],\n",
            "        [-2.5512e-02, -2.3318e-02, -7.2939e-03, -6.5648e-02,  9.3790e-02,\n",
            "         -3.6729e-02,  1.3050e-01,  5.4028e-02, -1.2578e-02],\n",
            "        [ 4.2424e-02, -6.5604e-02, -1.2618e-03,  6.2185e-03,  5.4778e-02,\n",
            "         -6.1648e-02,  1.1034e-01,  3.9774e-02,  2.4465e-02],\n",
            "        [-2.1616e-02,  9.1531e-02, -1.6602e-02, -1.1408e-01,  9.4129e-02,\n",
            "          8.3263e-02,  3.2855e-02,  1.6759e-04,  1.9460e-02]], device='cuda:0')\n",
            "Lipschitz Constant: 0.9976580739021301\n",
            "Difference: 88.5142822265625\n"
          ]
        }
      ],
      "source": [
        "#ADMM L1-LInfty\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#layer = alexnet_model.features[8].weight.to(device)\n",
        "layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U1 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U2 = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "H_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "U_bar = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "res = 100\n",
        "rho = 0.1\n",
        "\n",
        "i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while(res>0.05)\n",
        "#while pri_res>0.01:\n",
        "for i in range(200):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "\n",
        "    X1 =  Hf - U1\n",
        "    X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "    X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=2),min=1.0)\n",
        "    Hf1 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,1,s[1]))))\n",
        "    Hf1 = torch.reshape(torch.permute(Hf1,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "    X1 = Hf - U2\n",
        "    X1_mat = torch.permute(torch.reshape(X1,(s[0],s[1],n,n)),(2,3,0,1))\n",
        "    X1_norm = torch.clamp(torch.sum(torch.abs(X1_mat), dim=3),min=1.0)\n",
        "    Hf2 = torch.div(X1_mat,torch.reshape(X1_norm,((n,n,s[0],1))))\n",
        "    Hf2 = torch.reshape(torch.permute(Hf2,(2,3,0,1)),(s[0]*s[1],n*n))\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    dual_res = torch.norm(H_bar - (H_fourier + Hf1 + Hf2)/3,p='fro')**2\n",
        "    H_bar = (H_fourier + Hf1 + Hf2)/3\n",
        "    Hf = H_bar + U_bar\n",
        "\n",
        "    U = U + H_fourier - Hf\n",
        "    U1 = U1 + Hf1 - Hf\n",
        "    U2 = U2 + Hf2 - Hf\n",
        "    U_bar = (U1+U2+U)/3\n",
        "\n",
        "    pri_res = torch.norm(H_fourier - H_bar,p='fro')**2 + torch.norm(Hf1 - H_bar,p='fro')**2 + torch.norm(Hf2 - H_bar,p='fro')**2\n",
        "    #pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res},Dual Residual:{dual_res}')\n",
        "    \n",
        "    #i=i+1\n",
        "    \n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}') \n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference: {torch.norm((H - H0), p=\"fro\")**2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.995888888835907\n"
          ]
        }
      ],
      "source": [
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.77319067 0.72823359 0.91769675 0.30920708 0.20493604]\n",
            " [0.56429897 0.58296617 0.52434477 0.52120644 0.80432528]\n",
            " [0.21412907 0.2877764  0.75531956 0.01114734 0.52108853]]\n",
            "[[0.58932538 0.5285762  0.5185777  0.3092322  0.02813078]\n",
            " [0.38043675 0.38331079 0.1252202  0.52122915 0.62755118]\n",
            " [0.03023787 0.08811301 0.35620212 0.0111572  0.34431805]]\n"
          ]
        }
      ],
      "source": [
        "mat = np.random.rand(3,5)\n",
        "print(mat)\n",
        "\n",
        "x = cp.Variable((3,5))\n",
        "objective=cp.Minimize(cp.norm(x-mat,\"fro\"))\n",
        "constraints = [cp.norm(x,1)<=1]\n",
        "\n",
        "cp.Problem(objective,constraints).solve();\n",
        "print(x.value)\n",
        "\n",
        "y = np.sum(mat,axis=0)\n",
        "np.divide()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 25])\n",
            "torch.Size([1600, 25])\n",
            "torch.Size([12288, 25])\n",
            "tensor([[ 4.4748e-05,  1.7698e-05,  4.5946e-04,  ...,  3.8863e-04,\n",
            "          2.0615e-04, -5.8186e-05],\n",
            "        [-6.7374e-05, -2.3564e-04, -4.0030e-04,  ..., -2.7144e-05,\n",
            "         -2.0457e-04,  6.9573e-05],\n",
            "        [-1.5539e-04, -7.3378e-04, -1.1887e-03,  ..., -9.9195e-05,\n",
            "         -1.0101e-03,  1.7723e-04],\n",
            "        ...,\n",
            "        [-7.9884e-05, -4.0650e-04, -1.9423e-04,  ...,  5.4587e-06,\n",
            "          9.2837e-05,  1.3481e-04],\n",
            "        [ 2.5528e-04,  2.5600e-04,  9.4541e-04,  ..., -1.3024e-04,\n",
            "         -1.6855e-04, -3.0799e-05],\n",
            "        [ 8.6946e-04,  2.6536e-04, -1.1270e-04,  ..., -2.5305e-04,\n",
            "          2.1015e-04,  3.0038e-04]], device='cuda:0')\n",
            "Step 0: Primal Residual:0.0\n",
            "tensor([[ 8.8943e-05,  3.5178e-05,  9.1326e-04,  ...,  7.7247e-04,\n",
            "          4.0975e-04, -1.1565e-04],\n",
            "        [-1.3392e-04, -4.6836e-04, -7.9565e-04,  ..., -5.3954e-05,\n",
            "         -4.0661e-04,  1.3829e-04],\n",
            "        [-3.0886e-04, -1.4585e-03, -2.3626e-03,  ..., -1.9717e-04,\n",
            "         -2.0077e-03,  3.5226e-04],\n",
            "        ...,\n",
            "        [-1.5878e-04, -8.0797e-04, -3.8607e-04,  ...,  1.0850e-05,\n",
            "          1.8453e-04,  2.6795e-04],\n",
            "        [ 5.0741e-04,  5.0884e-04,  1.8791e-03,  ..., -2.5886e-04,\n",
            "         -3.3502e-04, -6.1217e-05],\n",
            "        [ 1.7282e-03,  5.2745e-04, -2.2401e-04,  ..., -5.0297e-04,\n",
            "          4.1770e-04,  5.9704e-04]], device='cuda:0')\n",
            "tensor([[ 1.4153e-04,  6.8283e-05,  1.3812e-03,  ...,  1.1831e-03,\n",
            "          6.3497e-04, -1.5846e-04],\n",
            "        [-2.1924e-04, -7.2231e-04, -1.2093e-03,  ..., -1.1374e-04,\n",
            "         -6.3676e-04,  1.8319e-04],\n",
            "        [-4.3078e-04, -2.1494e-03, -3.5048e-03,  ..., -3.2861e-04,\n",
            "         -3.0035e-03,  5.3625e-04],\n",
            "        ...,\n",
            "        [-1.8679e-04, -1.1493e-03, -5.2175e-04,  ..., -3.2857e-05,\n",
            "          2.2421e-04,  3.5279e-04],\n",
            "        [ 6.0296e-04,  5.4775e-04,  2.5621e-03,  ..., -5.2760e-04,\n",
            "         -6.3006e-04, -1.8636e-04],\n",
            "        [ 2.5979e-03,  8.4000e-04, -2.5542e-04,  ..., -6.6143e-04,\n",
            "          7.0942e-04,  9.5770e-04]], device='cuda:0')\n",
            "tensor([[-1.0317e-05, -1.2974e-04,  1.7027e-03,  ...,  1.5648e-03,\n",
            "          7.0318e-04, -3.6008e-04],\n",
            "        [-1.7487e-04, -9.1289e-04, -1.4808e-03,  ..., -1.1638e-04,\n",
            "         -7.6353e-04,  3.3733e-04],\n",
            "        [ 2.4324e-04, -1.9239e-03, -4.1055e-03,  ..., -5.5810e-04,\n",
            "         -3.3011e-03,  1.5112e-03],\n",
            "        ...,\n",
            "        [ 2.1156e-04, -8.9092e-04, -4.6891e-05,  ..., -3.5966e-04,\n",
            "         -1.7021e-05,  2.6332e-04],\n",
            "        [ 6.4280e-04, -3.6532e-05,  2.1546e-03,  ..., -1.0948e-03,\n",
            "         -9.9048e-04, -5.6854e-05],\n",
            "        [ 2.8573e-03,  8.5767e-04, -1.0811e-04,  ..., -5.2301e-04,\n",
            "          1.0745e-03,  1.0779e-03]], device='cuda:0')\n",
            "tensor([[-1.3419e-04, -3.5782e-04,  1.5654e-03,  ...,  1.4657e-03,\n",
            "          6.0984e-04, -6.2913e-04],\n",
            "        [ 7.5550e-05, -8.5530e-04, -1.4683e-03,  ...,  5.4625e-05,\n",
            "         -6.7605e-04,  4.7028e-04],\n",
            "        [ 6.8174e-04, -1.0279e-03, -4.1596e-03,  ..., -5.4080e-04,\n",
            "         -2.8098e-03,  2.2164e-03],\n",
            "        ...,\n",
            "        [ 4.6354e-04, -7.4685e-04,  1.7469e-04,  ..., -3.7697e-04,\n",
            "          7.0069e-05,  3.0840e-04],\n",
            "        [ 8.5666e-04, -2.0320e-04,  1.9668e-03,  ..., -1.3635e-03,\n",
            "         -7.9923e-04,  3.5205e-04],\n",
            "        [ 2.7075e-03,  5.5053e-04, -1.1205e-04,  ..., -5.9870e-04,\n",
            "          1.0209e-03,  8.6012e-04]], device='cuda:0')\n",
            "tensor([[-1.9602e-04, -2.1508e-04,  1.1066e-03,  ...,  1.2055e-03,\n",
            "          6.5215e-04, -7.4563e-04],\n",
            "        [ 2.1337e-04, -7.7676e-04, -1.4394e-03,  ...,  1.2088e-04,\n",
            "         -6.6714e-04,  4.4889e-04],\n",
            "        [ 1.2900e-04, -4.9264e-04, -4.0233e-03,  ..., -3.6546e-04,\n",
            "         -2.3980e-03,  1.7870e-03],\n",
            "        ...,\n",
            "        [ 4.5724e-04, -8.3523e-04,  1.9200e-05,  ..., -4.0394e-04,\n",
            "          2.1290e-04,  2.9969e-04],\n",
            "        [ 9.0624e-04, -1.8503e-04,  1.9717e-03,  ..., -1.4788e-03,\n",
            "         -5.1287e-04,  5.1228e-04],\n",
            "        [ 2.6036e-03,  3.8258e-04, -8.9408e-05,  ..., -6.7460e-04,\n",
            "          9.3709e-04,  6.9786e-04]], device='cuda:0')\n",
            "tensor([[-2.1724e-04, -1.7965e-05,  7.9604e-04,  ...,  1.1853e-03,\n",
            "          6.4311e-04, -6.6398e-04],\n",
            "        [ 2.2208e-04, -7.2353e-04, -1.4477e-03,  ...,  1.2436e-04,\n",
            "         -7.3098e-04,  4.4203e-04],\n",
            "        [-5.0005e-04, -4.2523e-04, -3.8677e-03,  ..., -7.1974e-05,\n",
            "         -2.3228e-03,  1.1657e-03],\n",
            "        ...,\n",
            "        [ 4.5273e-04, -9.3108e-04, -8.6366e-05,  ..., -4.8840e-04,\n",
            "          2.7027e-04,  2.7877e-04],\n",
            "        [ 9.2017e-04, -2.2558e-04,  2.0556e-03,  ..., -1.5396e-03,\n",
            "         -4.1314e-04,  5.0564e-04],\n",
            "        [ 2.6049e-03,  3.4988e-04, -3.6562e-05,  ..., -6.8839e-04,\n",
            "          9.4022e-04,  6.5090e-04]], device='cuda:0')\n",
            "tensor([[-2.1571e-04,  1.0243e-04,  6.9324e-04,  ...,  1.3762e-03,\n",
            "          5.0541e-04, -4.8029e-04],\n",
            "        [ 1.7115e-04, -6.7274e-04, -1.4677e-03,  ...,  1.1776e-04,\n",
            "         -7.8009e-04,  4.5719e-04],\n",
            "        [-7.8038e-04, -5.6961e-04, -3.8039e-03,  ...,  1.6943e-04,\n",
            "         -2.4512e-03,  8.3846e-04],\n",
            "        ...,\n",
            "        [ 4.8905e-04, -9.8744e-04, -7.5186e-05,  ..., -5.6555e-04,\n",
            "          2.9082e-04,  2.5601e-04],\n",
            "        [ 9.5729e-04, -3.0691e-04,  2.1410e-03,  ..., -1.5804e-03,\n",
            "         -4.1590e-04,  4.6047e-04],\n",
            "        [ 2.6101e-03,  3.5833e-04,  1.9709e-05,  ..., -6.7536e-04,\n",
            "          9.8617e-04,  6.2887e-04]], device='cuda:0')\n",
            "tensor([[-1.9741e-04,  1.3916e-04,  7.1010e-04,  ...,  1.6290e-03,\n",
            "          3.1933e-04, -2.9995e-04],\n",
            "        [ 1.1507e-04, -6.2497e-04, -1.4988e-03,  ...,  1.1828e-04,\n",
            "         -7.9953e-04,  4.8124e-04],\n",
            "        [-7.9725e-04, -7.2884e-04, -3.8224e-03,  ...,  3.3529e-04,\n",
            "         -2.6319e-03,  7.7822e-04],\n",
            "        ...,\n",
            "        [ 5.4234e-04, -1.0189e-03, -6.8169e-06,  ..., -6.2021e-04,\n",
            "          3.0980e-04,  2.4026e-04],\n",
            "        [ 1.0125e-03, -3.8939e-04,  2.2106e-03,  ..., -1.6315e-03,\n",
            "         -4.3231e-04,  4.3637e-04],\n",
            "        [ 2.6022e-03,  3.6608e-04,  6.7336e-05,  ..., -6.6292e-04,\n",
            "          1.0360e-03,  5.9558e-04]], device='cuda:0')\n",
            "tensor([[-1.8125e-04,  1.3551e-04,  7.5919e-04,  ...,  1.8463e-03,\n",
            "          1.4752e-04, -1.6750e-04],\n",
            "        [ 7.4188e-05, -5.8001e-04, -1.5359e-03,  ...,  1.2692e-04,\n",
            "         -8.0115e-04,  5.0462e-04],\n",
            "        [-7.0822e-04, -8.2913e-04, -3.8772e-03,  ...,  4.5262e-04,\n",
            "         -2.7846e-03,  8.3728e-04],\n",
            "        ...,\n",
            "        [ 5.8834e-04, -1.0439e-03,  6.4325e-05,  ..., -6.5930e-04,\n",
            "          3.4003e-04,  2.3152e-04],\n",
            "        [ 1.0706e-03, -4.5163e-04,  2.2628e-03,  ..., -1.6945e-03,\n",
            "         -4.2157e-04,  4.3657e-04],\n",
            "        [ 2.5870e-03,  3.6077e-04,  1.0222e-04,  ..., -6.6184e-04,\n",
            "          1.0695e-03,  5.5135e-04]], device='cuda:0')\n",
            "Optimized H:\n",
            "tensor([[-1.8125e-04,  1.3551e-04,  7.5919e-04,  ...,  1.8463e-03,\n",
            "          1.4752e-04, -1.6750e-04],\n",
            "        [ 7.4188e-05, -5.8001e-04, -1.5359e-03,  ...,  1.2692e-04,\n",
            "         -8.0115e-04,  5.0462e-04],\n",
            "        [-7.0822e-04, -8.2913e-04, -3.8772e-03,  ...,  4.5262e-04,\n",
            "         -2.7846e-03,  8.3728e-04],\n",
            "        ...,\n",
            "        [ 5.8834e-04, -1.0439e-03,  6.4325e-05,  ..., -6.5930e-04,\n",
            "          3.4003e-04,  2.3152e-04],\n",
            "        [ 1.0706e-03, -4.5163e-04,  2.2628e-03,  ..., -1.6945e-03,\n",
            "         -4.2157e-04,  4.3657e-04],\n",
            "        [ 2.5870e-03,  3.6077e-04,  1.0222e-04,  ..., -6.6184e-04,\n",
            "          1.0695e-03,  5.5135e-04]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "all_one = torch.ones(n*n,device=device)\n",
        "H0 = torch.reshape(layer,(s[0]*s[1],s[2]*s[3])).to(device)\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "#i = 0\n",
        "#while pri_res>0.5:\n",
        "for i in range(10):\n",
        "    # Optimize H\n",
        "    H = SGDminimize(H0,Hf,U,F,rho)\n",
        "    print(H)\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "\n",
        "    #Optimize Hf\n",
        "\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,s_f[0])))\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.583524763584137\n",
            "Difference:637.15673828125\n",
            "Original H:\n",
            "tensor([[ 0.0036,  0.0014,  0.0372,  ...,  0.0315,  0.0167, -0.0047],\n",
            "        [-0.0055, -0.0191, -0.0324,  ..., -0.0022, -0.0166,  0.0056],\n",
            "        [-0.0126, -0.0594, -0.0963,  ..., -0.0080, -0.0818,  0.0144],\n",
            "        ...,\n",
            "        [-0.0065, -0.0329, -0.0157,  ...,  0.0004,  0.0075,  0.0109],\n",
            "        [ 0.0207,  0.0207,  0.0766,  ..., -0.0105, -0.0137, -0.0025],\n",
            "        [ 0.0704,  0.0215, -0.0091,  ..., -0.0205,  0.0170,  0.0243]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p=\"fro\")**2}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9353"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 1600])\n",
            "torch.Size([12288, 1600])\n",
            "Step 0: Primal Residual:0.0 --------- Dual Residual:12.910954475402832\n",
            "Step 10: Primal Residual:0.5220763683319092 --------- Dual Residual:0.9866501092910767\n",
            "Step 20: Primal Residual:0.054465826600790024 --------- Dual Residual:0.09499482065439224\n",
            "Step 30: Primal Residual:0.011437932029366493 --------- Dual Residual:0.015981443226337433\n",
            "Step 40: Primal Residual:0.003817094024270773 --------- Dual Residual:0.003910079598426819\n",
            "Step 50: Primal Residual:0.0012178265023976564 --------- Dual Residual:0.001061644172295928\n",
            "Step 60: Primal Residual:0.0004802555777132511 --------- Dual Residual:0.00035684873000718653\n",
            "Step 70: Primal Residual:0.00022325586178340018 --------- Dual Residual:0.00016058441542554647\n",
            "Step 80: Primal Residual:0.00010772810492198914 --------- Dual Residual:7.935526082292199e-05\n",
            "Step 90: Primal Residual:5.301719647832215e-05 --------- Dual Residual:3.978165841544978e-05\n",
            "Optimized H:\n",
            "tensor([[ 3.3273e-03, -4.0544e-03,  2.3649e-03,  ...,  0.0000e+00,\n",
            "          8.6880e-04, -2.2530e-04],\n",
            "        [ 1.2119e-03,  1.0202e-04,  1.1287e-03,  ...,  0.0000e+00,\n",
            "         -5.1178e-04,  6.4885e-04],\n",
            "        [ 9.8308e-03,  8.8841e-04, -3.3058e-03,  ...,  0.0000e+00,\n",
            "          4.2865e-04, -4.6647e-03],\n",
            "        ...,\n",
            "        [ 1.4153e-03,  6.6036e-04,  1.3784e-03,  ...,  0.0000e+00,\n",
            "         -2.1747e-03, -1.3814e-03],\n",
            "        [ 2.9316e-03,  1.0410e-03,  9.8553e-04,  ...,  0.0000e+00,\n",
            "         -7.4977e-04,  1.3376e-03],\n",
            "        [-2.2959e-03, -1.0089e-03, -2.9629e-04,  ...,  0.0000e+00,\n",
            "          8.1403e-04,  6.7341e-06]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize - Heuristc\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "F = createFourierMatrix(n,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "H0 = zeroPad2DMatrix(layer,n).detach()\n",
        "H0 = torch.reshape(H0,(s[0]*s[1],n*n)).to(device)\n",
        "\n",
        "print(H0.shape)\n",
        "#F = F.to(device)\n",
        "#print(F.shape)\n",
        "#F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "temp = 2*torch.eye(n*n).to(device) + rho*torch.real(F.H@F)\n",
        "temp = temp.to('cpu')\n",
        "inv_mat = torch.linalg.inv(temp)\n",
        "inv_mat = inv_mat.to(device)\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "\n",
        "    x = (Hf-U)@torch.conj(F)\n",
        "    H = (2*H0 + (rho) * torch.real(x))@(inv_mat)\n",
        "    H = torch.reshape(H,(s[0],s[1],n,n))\n",
        "    H = deZeroPad2DMatrix(H,k)\n",
        "    H = zeroPad2DMatrix(H,n)\n",
        "    H = torch.reshape(H,(s[0]*s[1],n*n))\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "\n",
        "    dual_res = Hf\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    #H_frob = torch.sum(torch.abs(Hf),dim = 0)\n",
        "    #print(H_frob)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,s_f[0])))\n",
        "    dual_res = torch.norm(Hf - dual_res, p='fro')\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res} --------- Dual Residual:{dual_res}')\n",
        "    \n",
        "    del x,H_fourier,s_f\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.5835258960723877\n",
            "Difference:637.15673828125\n",
            "Original H:\n",
            "tensor([[ 0.0216, -0.0695,  0.0316,  ...,  0.0000,  0.0241, -0.0121],\n",
            "        [ 0.0613,  0.0133,  0.0262,  ...,  0.0000, -0.0075,  0.0309],\n",
            "        [ 0.2158,  0.0295, -0.0589,  ...,  0.0000, -0.0150, -0.1121],\n",
            "        ...,\n",
            "        [ 0.0448,  0.0221,  0.0354,  ...,  0.0000, -0.0541, -0.0547],\n",
            "        [ 0.1288,  0.0619,  0.0294,  ...,  0.0000, -0.0010,  0.0554],\n",
            "        [-0.0856, -0.0549, -0.0188,  ...,  0.0000,  0.0309, -0.0101]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "A = torch.reshape(H,(s[0],s[1],n,n))\n",
        "A = deZeroPad2DMatrix(A,k)\n",
        "layer_wt = torch.reshape(A,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p=\"fro\")**2}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 25])\n",
            "torch.Size([12288, 25])\n",
            "Step 0: Primal Residual:0.0 --- Dual Residual:12.910954475402832\n",
            "Step 10: Primal Residual:0.5220803022384644 --- Dual Residual:0.9866498112678528\n",
            "Step 20: Primal Residual:0.05446581169962883 --- Dual Residual:0.094994455575943\n",
            "Step 30: Primal Residual:0.011437954381108284 --- Dual Residual:0.015981435775756836\n",
            "Step 40: Primal Residual:0.003816974116489291 --- Dual Residual:0.003910013474524021\n",
            "Step 50: Primal Residual:0.001217905431985855 --- Dual Residual:0.001061499584466219\n",
            "Step 60: Primal Residual:0.0004802561306860298 --- Dual Residual:0.00035629564081318676\n",
            "Step 70: Primal Residual:0.00022331906075123698 --- Dual Residual:0.00015967278159223497\n",
            "Step 80: Primal Residual:0.00010823555203387514 --- Dual Residual:7.790025847498327e-05\n",
            "Step 90: Primal Residual:5.3875643061473966e-05 --- Dual Residual:3.769535032915883e-05\n",
            "Optimized H:\n",
            "tensor([[-1.3102e-04, -5.5373e-05,  1.0151e-03,  ...,  1.9139e-03,\n",
            "          5.0321e-05, -1.3983e-04],\n",
            "        [ 1.0306e-05, -5.1918e-04, -1.6158e-03,  ...,  1.6130e-04,\n",
            "         -8.1378e-04,  4.8280e-04],\n",
            "        [-4.8498e-04, -7.6491e-04, -3.9981e-03,  ...,  4.7659e-04,\n",
            "         -2.9041e-03,  1.1253e-03],\n",
            "        ...,\n",
            "        [ 5.5528e-04, -1.0779e-03,  1.0411e-04,  ..., -6.7455e-04,\n",
            "          4.0486e-04,  1.5434e-04],\n",
            "        [ 1.0124e-03, -4.1077e-04,  2.2555e-03,  ..., -1.7933e-03,\n",
            "         -2.5562e-04,  3.5951e-04],\n",
            "        [ 2.5881e-03,  3.5448e-04,  9.4994e-05,  ..., -6.8891e-04,\n",
            "          1.1016e-03,  4.8659e-04]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize - Exact\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "\n",
        "H0 = torch.reshape(layer.detach(),(s[0]*s[1],k*k)).to(device)\n",
        "\n",
        "print(H0.shape)\n",
        "F = F.to(device)\n",
        "#print(F.shape)\n",
        "F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,requires_grad=False,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),requires_grad=False,dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),requires_grad=False,dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "temp = 2*torch.eye(k*k).to(device) + rho*torch.real(F.H@F)\n",
        "temp = temp.to('cpu')\n",
        "inv_mat = torch.linalg.inv(temp)\n",
        "inv_mat = inv_mat.to(device)\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "    x = Hf - U\n",
        "    H = (2*H0 + rho*torch.real(torch.conj(x)@F))@inv_mat\n",
        "    #H = H.to(torch.float32)\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "    #print(H_fourier)\n",
        "    #Optimize Hf\n",
        "\n",
        "    dual_res = Hf\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    #H_frob = torch.sum(torch.abs(Hf),dim = 0)\n",
        "    #print(H_frob)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,s_f[0])))\n",
        "    dual_res = torch.norm(Hf - dual_res, p='fro')\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res} --- Dual Residual:{dual_res}')\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Output Test\n",
        "layer_wt = torch.reshape(H,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p=\"fro\")**2}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 1600])\n",
            "torch.Size([12288, 1600])\n",
            "tensor([[ 1.6000e+03, -3.0037e-04,  1.4882e-04,  ..., -4.0298e-06,\n",
            "         -9.7487e-06,  6.5536e-06],\n",
            "        [-2.9931e-04,  1.6000e+03, -7.3511e-06,  ..., -1.2481e-06,\n",
            "         -9.5871e-07,  2.7905e-06],\n",
            "        [ 1.4882e-04, -6.6557e-06,  1.6000e+03,  ..., -2.4866e-06,\n",
            "          4.8422e-06, -5.9385e-07],\n",
            "        ...,\n",
            "        [-6.2476e-06, -1.2481e-06, -3.3441e-07,  ...,  1.6000e+03,\n",
            "          5.6859e-05,  2.6223e-05],\n",
            "        [-9.7487e-06,  2.7772e-06,  4.8422e-06,  ...,  5.5696e-05,\n",
            "          1.6000e+03,  2.0068e-04],\n",
            "        [ 8.4459e-06,  2.7905e-06, -6.8718e-08,  ...,  2.6223e-05,\n",
            "          2.0295e-04,  1.6000e+03]], device='cuda:0')\n",
            "Step 0: Primal Residual:0.0 --------- Dual Residual:12.910962104797363\n",
            "Step 10: Primal Residual:0.5220791697502136 --------- Dual Residual:0.9866526126861572\n",
            "Step 20: Primal Residual:0.0544651560485363 --------- Dual Residual:0.09499283879995346\n",
            "Step 30: Primal Residual:0.011437925510108471 --------- Dual Residual:0.01598128117620945\n",
            "Step 40: Primal Residual:0.0038170390762388706 --------- Dual Residual:0.003910110332071781\n",
            "Step 50: Primal Residual:0.0012178737670183182 --------- Dual Residual:0.0010615710634738207\n",
            "Step 60: Primal Residual:0.00048027673619799316 --------- Dual Residual:0.00035679072607308626\n",
            "Step 70: Primal Residual:0.00022320810239762068 --------- Dual Residual:0.00016058266919571906\n",
            "Step 80: Primal Residual:0.00010777667193906382 --------- Dual Residual:7.926228863652796e-05\n",
            "Step 90: Primal Residual:5.3030336857773364e-05 --------- Dual Residual:3.975290746893734e-05\n",
            "Optimized H:\n",
            "tensor([[ 3.3273e-03, -4.0544e-03,  2.3649e-03,  ...,  0.0000e+00,\n",
            "          8.6880e-04, -2.2530e-04],\n",
            "        [ 1.2119e-03,  1.0202e-04,  1.1287e-03,  ...,  0.0000e+00,\n",
            "         -5.1178e-04,  6.4885e-04],\n",
            "        [ 9.8309e-03,  8.8840e-04, -3.3058e-03,  ...,  0.0000e+00,\n",
            "          4.2865e-04, -4.6647e-03],\n",
            "        ...,\n",
            "        [ 1.4153e-03,  6.6035e-04,  1.3784e-03,  ...,  0.0000e+00,\n",
            "         -2.1747e-03, -1.3815e-03],\n",
            "        [ 2.9316e-03,  1.0410e-03,  9.8553e-04,  ...,  0.0000e+00,\n",
            "         -7.4977e-04,  1.3376e-03],\n",
            "        [-2.2959e-03, -1.0089e-03, -2.9630e-04,  ...,  0.0000e+00,\n",
            "          8.1404e-04,  6.7325e-06]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize - Heuristc\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "F = createFourierMatrix(n,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "H0 = zeroPad2DMatrix(layer,n).detach()\n",
        "H0 = torch.reshape(H0,(s[0]*s[1],n*n)).to(device)\n",
        "\n",
        "print(H0.shape)\n",
        "#F = F.to(device)\n",
        "#print(F.shape)\n",
        "#F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n*n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.1\n",
        "\n",
        "temp = 2*torch.eye(n*n).to(device) + rho*torch.real(F.H@F)\n",
        "temp = temp.to('cpu')\n",
        "inv_mat = torch.linalg.inv(temp)\n",
        "inv_mat = inv_mat.to(device)\n",
        "print(torch.real(F.H@F))\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(100):\n",
        "    # Optimize H\n",
        "\n",
        "    x = (Hf-U)@torch.conj(F)\n",
        "    #H = (2*H0 + (rho) * torch.real(x))@(inv_mat)\n",
        "    H = (2*H0 + (rho) * torch.real(x))/(2+rho*n*n)\n",
        "    H = torch.reshape(H,(s[0],s[1],n,n))\n",
        "    H = deZeroPad2DMatrix(H,k)\n",
        "    H = zeroPad2DMatrix(H,n)\n",
        "    H = torch.reshape(H,(s[0]*s[1],n*n))\n",
        "\n",
        "    H_fourier = torch.zeros(Hf.shape,dtype = torch.complex64).to(device)\n",
        "    H_fourier.real = H@torch.real(F.T)\n",
        "    H_fourier.imag = H@torch.imag(F.T)\n",
        "\n",
        "    dual_res = Hf\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    #H_frob = torch.sum(torch.abs(Hf),dim = 0)\n",
        "    #print(H_frob)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,s_f[0])))\n",
        "    dual_res = torch.norm(Hf - dual_res, p='fro')\n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%10 == 0):\n",
        "        print(f'Step {i}: Primal Residual:{pri_res} --------- Dual Residual:{dual_res}')\n",
        "    \n",
        "    del x,H_fourier,s_f\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.5835268497467041\n",
            "Difference:637.15673828125\n",
            "Original H:\n",
            "tensor([[ 0.0216, -0.0695,  0.0316,  ...,  0.0000,  0.0241, -0.0121],\n",
            "        [ 0.0613,  0.0133,  0.0262,  ...,  0.0000, -0.0075,  0.0309],\n",
            "        [ 0.2158,  0.0295, -0.0589,  ...,  0.0000, -0.0150, -0.1121],\n",
            "        ...,\n",
            "        [ 0.0448,  0.0221,  0.0354,  ...,  0.0000, -0.0541, -0.0547],\n",
            "        [ 0.1288,  0.0619,  0.0294,  ...,  0.0000, -0.0010,  0.0554],\n",
            "        [-0.0856, -0.0549, -0.0188,  ...,  0.0000,  0.0309, -0.0101]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "A = torch.reshape(H,(s[0],s[1],n,n))\n",
        "A = deZeroPad2DMatrix(A,k)\n",
        "layer_wt = torch.reshape(A,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p=\"fro\")**2}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12288, 40, 40])\n",
            "torch.Size([12288, 40, 40])\n",
            "Step 0: Residual:80.98329162597656\n",
            "Step 100: Residual:0.03360621631145477\n",
            "Step 200: Residual:0.01159454695880413\n",
            "Step 300: Residual:0.0059705800376832485\n",
            "Step 400: Residual:0.0033911247737705708\n",
            "Step 500: Residual:0.0019716101232916117\n",
            "Step 600: Residual:0.0011520414846017957\n",
            "Step 700: Residual:0.0006738461670465767\n",
            "Step 800: Residual:0.00039428286254405975\n",
            "Step 900: Residual:0.00023074196360539645\n",
            "Optimized H:\n",
            "tensor([[[ 3.3273e-03, -4.0544e-03,  2.3649e-03,  ...,  0.0000e+00,\n",
            "           2.1686e-03, -5.9101e-03],\n",
            "         [-3.7785e-03,  2.9641e-03, -1.0997e-03,  ...,  0.0000e+00,\n",
            "          -7.4911e-04,  6.0331e-04],\n",
            "         [ 1.9139e-03,  5.0315e-05, -1.3983e-04,  ...,  0.0000e+00,\n",
            "          -5.2857e-04,  1.3500e-04],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 1.0151e-03, -3.3122e-04, -3.8046e-05,  ...,  0.0000e+00,\n",
            "          -1.3103e-04, -5.5375e-05],\n",
            "         [ 3.2546e-03, -3.8184e-03,  9.2159e-04,  ...,  0.0000e+00,\n",
            "           8.6880e-04, -2.2530e-04]],\n",
            "\n",
            "        [[ 1.2119e-03,  1.0202e-04,  1.1287e-03,  ...,  0.0000e+00,\n",
            "          -2.4417e-04,  3.0046e-03],\n",
            "         [ 1.4802e-03, -5.4318e-04, -2.1073e-05,  ...,  0.0000e+00,\n",
            "          -1.3874e-03, -1.2298e-04],\n",
            "         [ 1.6129e-04, -8.1377e-04,  4.8280e-04,  ...,  0.0000e+00,\n",
            "           8.6545e-04, -1.8293e-03],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-1.6158e-03, -4.0001e-04, -1.4939e-04,  ...,  0.0000e+00,\n",
            "           1.0324e-05, -5.1918e-04],\n",
            "         [ 1.3255e-03, -4.5301e-04, -7.7191e-04,  ...,  0.0000e+00,\n",
            "          -5.1179e-04,  6.4885e-04]],\n",
            "\n",
            "        [[ 9.8308e-03,  8.8841e-04, -3.3058e-03,  ...,  0.0000e+00,\n",
            "          -6.0402e-04, -9.9049e-05],\n",
            "         [-9.4379e-04, -5.7420e-03,  2.5437e-04,  ...,  0.0000e+00,\n",
            "          -5.1889e-04,  4.3177e-03],\n",
            "         [ 4.7655e-04, -2.9041e-03,  1.1252e-03,  ...,  0.0000e+00,\n",
            "           1.0632e-03,  9.3067e-04],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [-3.9981e-03,  1.6434e-03,  9.7628e-04,  ...,  0.0000e+00,\n",
            "          -4.8496e-04, -7.6489e-04],\n",
            "         [-1.5128e-03,  8.1603e-04,  2.2337e-03,  ...,  0.0000e+00,\n",
            "           4.2865e-04, -4.6647e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.4153e-03,  6.6035e-04,  1.3784e-03,  ...,  0.0000e+00,\n",
            "           1.2555e-03,  9.5106e-04],\n",
            "         [ 2.1857e-03, -2.7593e-04,  5.2487e-04,  ...,  0.0000e+00,\n",
            "           3.8928e-04, -1.2007e-04],\n",
            "         [-6.7455e-04,  4.0487e-04,  1.5433e-04,  ...,  0.0000e+00,\n",
            "          -3.0274e-04, -1.7819e-04],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 1.0411e-04, -4.3155e-04,  6.0284e-04,  ...,  0.0000e+00,\n",
            "           5.5529e-04, -1.0779e-03],\n",
            "         [-1.4691e-03, -1.7350e-03, -1.3778e-03,  ...,  0.0000e+00,\n",
            "          -2.1747e-03, -1.3814e-03]],\n",
            "\n",
            "        [[ 2.9315e-03,  1.0410e-03,  9.8553e-04,  ...,  0.0000e+00,\n",
            "          -6.8111e-04,  2.4492e-04],\n",
            "         [ 5.1707e-03,  7.8577e-05, -1.4172e-03,  ...,  0.0000e+00,\n",
            "           1.5156e-03,  4.5226e-05],\n",
            "         [-1.7933e-03, -2.5563e-04,  3.5955e-04,  ...,  0.0000e+00,\n",
            "          -1.1054e-03, -6.5157e-05],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 2.2555e-03, -2.6132e-04,  1.5939e-03,  ...,  0.0000e+00,\n",
            "           1.0124e-03, -4.1076e-04],\n",
            "         [ 3.5937e-03,  1.3471e-03,  6.2803e-04,  ...,  0.0000e+00,\n",
            "          -7.4976e-04,  1.3376e-03]],\n",
            "\n",
            "        [[-2.2959e-03, -1.0089e-03, -2.9632e-04,  ...,  0.0000e+00,\n",
            "          -8.7444e-05, -8.8855e-04],\n",
            "         [-1.6275e-03, -1.1671e-03,  5.8678e-04,  ...,  0.0000e+00,\n",
            "          -3.9689e-04, -1.1168e-03],\n",
            "         [-6.8891e-04,  1.1016e-03,  4.8675e-04,  ...,  0.0000e+00,\n",
            "           1.8739e-03,  3.8820e-04],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 9.4982e-05, -2.0593e-04, -3.7292e-04,  ...,  0.0000e+00,\n",
            "           2.5882e-03,  3.5447e-04],\n",
            "         [-1.6036e-03, -7.1631e-04, -5.0759e-04,  ...,  0.0000e+00,\n",
            "           8.1403e-04,  6.7333e-06]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#ADMM Frobenius Normalize - Heuristc\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "layer = alexnet_model.features[3].weight.to(device)\n",
        "#layer = layer.to(device)\n",
        "s = layer.shape\n",
        "k = s[3]\n",
        "n = 40\n",
        "\n",
        "#F = createFourierMatrix(k,n)\n",
        "\n",
        "# Constants (example values, replace these with actual data)\n",
        "H0 = zeroPad2DMatrix(layer,n).detach()\n",
        "H0 = torch.reshape(H0,(s[0]*s[1],n,n)).to(device)\n",
        "\n",
        "print(H0.shape)\n",
        "#F = F.to(device)\n",
        "#print(F.shape)\n",
        "#F_real, F_imag = torch.real(F), torch.imag(F)\n",
        "\n",
        "# Initialize optimization variables (H and lambda)\n",
        "H = torch.rand_like(H0,dtype = torch.float32).to(device)\n",
        "print(H.shape)\n",
        "U = torch.zeros((s[0]*s[1],n,n),dtype=torch.complex64).to(device)\n",
        "Hf = torch.zeros((s[0]*s[1],n,n),dtype=torch.complex64).to(device)\n",
        "\n",
        "pri_res = 100\n",
        "rho = 0.01\n",
        "\n",
        "#i = 0\n",
        "#optimizer = torch.optim.Adam([H], lr=0.01)\n",
        "#while pri_res>0.5:\n",
        "for i in range(1000):\n",
        "    # Optimize H\n",
        "    x = torch.fft.ifft2(Hf-U)\n",
        "    H = (2*H0 + (rho) * torch.real(x)*n*n)/(2+rho*n*n)\n",
        "    H = torch.reshape(H,(s[0],s[1],n,n))\n",
        "    H = deZeroPad2DMatrix(H,k)\n",
        "    H = zeroPad2DMatrix(H,n)\n",
        "    H = torch.reshape(H,(s[0]*s[1],n,n))\n",
        "\n",
        "    H_fourier = torch.fft.fft2(H)\n",
        "\n",
        "    #Optimize Hf\n",
        "\n",
        "    dual_res = Hf\n",
        "    Hf = H_fourier + U\n",
        "    H_frob = torch.clamp(torch.sqrt(torch.sum(torch.square(torch.abs(Hf)),dim = 0)),min=1)\n",
        "    #H_frob = torch.sum(torch.abs(Hf),dim = 0)\n",
        "    #print(H_frob)\n",
        "    s_f = H_frob.shape\n",
        "    Hf = torch.div(Hf,torch.reshape(H_frob,(1,n,n)))\n",
        "    dual_res = torch.norm(torch.fft.ifft2(Hf - dual_res), p='fro')*rho\n",
        "    \n",
        "\n",
        "    # Update U\n",
        "    U = U + H_fourier - Hf\n",
        "    pri_res = torch.norm(H_fourier - Hf, p='fro')\n",
        "    #i=i+1\n",
        "    if(i%100 == 0):\n",
        "        print(f'Step {i}: Residual:{pri_res+dual_res}')\n",
        "    \n",
        "    del x,H_fourier,s_f\n",
        "# Results\n",
        "print(f'Optimized H:\\n{H}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lipschitz Constant: 0.583531379699707\n",
            "Difference:637.15673828125\n",
            "Original H:\n",
            "tensor([[[ 0.0216, -0.0695,  0.0316,  ...,  0.0000,  0.0210, -0.1062],\n",
            "         [-0.0466,  0.0221, -0.0013,  ...,  0.0000, -0.0083, -0.0390],\n",
            "         [ 0.0315,  0.0167, -0.0047,  ...,  0.0000, -0.0154,  0.0087],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0372, -0.0209,  0.0018,  ...,  0.0000,  0.0036,  0.0014],\n",
            "         [ 0.0712, -0.0852,  0.0131,  ...,  0.0000,  0.0241, -0.0121]],\n",
            "\n",
            "        [[ 0.0613,  0.0133,  0.0262,  ...,  0.0000,  0.0055,  0.0803],\n",
            "         [ 0.0384, -0.0063,  0.0040,  ...,  0.0000, -0.0269,  0.0059],\n",
            "         [-0.0022, -0.0166,  0.0056,  ...,  0.0000,  0.0021, -0.0363],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.0324, -0.0220, -0.0121,  ...,  0.0000, -0.0055, -0.0191],\n",
            "         [ 0.0319, -0.0076, -0.0147,  ...,  0.0000, -0.0075,  0.0309]],\n",
            "\n",
            "        [[ 0.2158,  0.0295, -0.0589,  ...,  0.0000, -0.0138,  0.0412],\n",
            "         [ 0.0219, -0.1401, -0.0313,  ...,  0.0000,  0.0123,  0.1009],\n",
            "         [-0.0080, -0.0818,  0.0144,  ...,  0.0000,  0.0378,  0.0526],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.0963,  0.0167,  0.0475,  ...,  0.0000, -0.0126, -0.0594],\n",
            "         [-0.0239,  0.0335,  0.0413,  ...,  0.0000, -0.0150, -0.1121]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0448,  0.0221,  0.0354,  ...,  0.0000,  0.0283,  0.0295],\n",
            "         [ 0.0546,  0.0171,  0.0234,  ...,  0.0000,  0.0168,  0.0166],\n",
            "         [ 0.0004,  0.0075,  0.0109,  ...,  0.0000, -0.0041, -0.0080],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.0157, -0.0178,  0.0021,  ...,  0.0000, -0.0065, -0.0329],\n",
            "         [-0.0454, -0.0563, -0.0367,  ...,  0.0000, -0.0541, -0.0547]],\n",
            "\n",
            "        [[ 0.1288,  0.0619,  0.0294,  ...,  0.0000, -0.0039,  0.0357],\n",
            "         [ 0.1325,  0.0327, -0.0180,  ...,  0.0000,  0.0260,  0.0386],\n",
            "         [-0.0105, -0.0137, -0.0025,  ...,  0.0000, -0.0199, -0.0139],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0766,  0.0291,  0.0416,  ...,  0.0000,  0.0207,  0.0207],\n",
            "         [ 0.1323,  0.0680,  0.0415,  ...,  0.0000, -0.0010,  0.0554]],\n",
            "\n",
            "        [[-0.0856, -0.0549, -0.0188,  ...,  0.0000, -0.0076, -0.0439],\n",
            "         [-0.0703, -0.0395,  0.0028,  ...,  0.0000, -0.0057, -0.0425],\n",
            "         [-0.0205,  0.0170,  0.0243,  ...,  0.0000,  0.0491,  0.0081],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.0091, -0.0091, -0.0057,  ...,  0.0000,  0.0704,  0.0215],\n",
            "         [-0.0584, -0.0388, -0.0202,  ...,  0.0000,  0.0309, -0.0101]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#Output Test\n",
        "A = torch.reshape(H,(s[0],s[1],n,n))\n",
        "A = deZeroPad2DMatrix(A,k)\n",
        "layer_wt = torch.reshape(A,(s[0],s[1],s[2],s[3]))\n",
        "print(f'Lipschitz Constant: {computeLayerLipschitzFourier(layer_wt,n)}')\n",
        "print(f'Difference:{torch.norm(H - H0, p=\"fro\")**2}')\n",
        "print(f'Original H:\\n{H0}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
