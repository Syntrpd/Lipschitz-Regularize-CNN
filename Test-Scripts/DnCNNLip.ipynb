{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmath\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, depth=17, n_channels=64, image_channels=1, use_bnorm=True, kernel_size=3):\n",
    "        super(DnCNN, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(depth-2):\n",
    "            layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(n_channels, eps=0.0001, momentum = 0.95))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        out = self.dncnn(x)\n",
    "        return y-out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.orthogonal_(m.weight)\n",
    "                print('init weight')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFourierMatrix(k,n):\n",
    "    i = cmath.sqrt(-1)\n",
    "    val = cmath.exp(-2*cmath.pi*i/n)\n",
    "    p = (k-1)/2\n",
    "    q = (k+1)/2\n",
    "    F = torch.zeros(n*n,k*k)\n",
    "    F = F.type(torch.complex64)\n",
    "\n",
    "    f = torch.zeros(n,1);\n",
    "    f = f.type(torch.complex64)\n",
    "    f_u = torch.zeros(n*n,1);\n",
    "    f_u = f_u.type(torch.complex64)\n",
    "    for u in range(n):\n",
    "        index = torch.arange(u*n,(u+1)*n)\n",
    "        f_u[u*n:(u+1)*n]=val**u\n",
    "        f[u]=val**u;\n",
    "\n",
    "    f_v = f.repeat(n,1);\n",
    "    for u in range(k):\n",
    "        for v in range(k):\n",
    "            a=0\n",
    "            b=0\n",
    "            if(u<=p):\n",
    "                a = n-p+u;\n",
    "            else:\n",
    "                a = u-p;\n",
    "\n",
    "\n",
    "            if(v<=p):\n",
    "                b = n-p+v;\n",
    "            else:\n",
    "                b = v-p;\n",
    "\n",
    "            F[:,(u*k+v)]=((torch.pow(f_u,(a)))*(torch.pow(f_v,(b)))).flatten();\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPad2DMatrix(layer_wt,n):\n",
    "    k = layer_wt.size()[3]\n",
    "    p = (k-1)/2\n",
    "    q = (k+1)/2\n",
    "    I = torch.eye(n);\n",
    "    ind1 = torch.arange(0,p)\n",
    "    ind2 = torch.arange(p,k)\n",
    "    ind3 = torch.arange(k,n)\n",
    "    indices = torch.cat((ind2,ind3,ind1))\n",
    "    indices=indices.type(torch.int64)\n",
    "    perm = I[indices];\n",
    "    perm_mat = perm.unsqueeze(0).unsqueeze(0)\n",
    "    pad_left = 0\n",
    "    pad_right = n - k\n",
    "    pad_top = 0\n",
    "    pad_bottom = n - k\n",
    "    # Apply padding\n",
    "    padded_wt = torch.nn.functional.pad(layer_wt, (pad_left, pad_right, pad_top, pad_bottom))\n",
    "    perm_mat_tr = torch.transpose(perm_mat,2,3)\n",
    "    padded_final = torch.matmul(perm_mat,torch.matmul(padded_wt,perm_mat_tr))\n",
    "    return padded_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLayerLipschitzFourier(layer_wt,n):\n",
    "    layer_wt_padded = zeroPad2DMatrix(layer_wt,n)\n",
    "    layer_pf=torch.fft.fft2(layer_wt_padded)\n",
    "    layer_fperm = torch.permute(layer_pf,(2,3,0,1))\n",
    "    sing = torch.linalg.svdvals(layer_fperm)\n",
    "    lip = torch.max(torch.abs(sing))\n",
    "    return lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/kunallab/sayan/DnCNN/DnCNN/TrainingCodes/dncnn_pytorch/models/DnCNN_sigma25/model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/serialization.py:1079: UserWarning: Couldn't retrieve source code for container of type DnCNN. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/kunallab/anaconda3/envs/sayan/lib/python3.12/site-packages/torch/serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n"
     ]
    }
   ],
   "source": [
    "net = DnCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DnCNN(\n",
      "  (dncnn): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (18): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (21): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (30): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (33): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (34): ReLU(inplace=True)\n",
      "    (35): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (36): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (37): ReLU(inplace=True)\n",
      "    (38): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (39): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (40): ReLU(inplace=True)\n",
      "    (41): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (42): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (43): ReLU(inplace=True)\n",
      "    (44): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (45): BatchNorm2d(64, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (46): ReLU(inplace=True)\n",
      "    (47): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51170.0820, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lip =1 \n",
    "for layer in net.dncnn:\n",
    "    if isinstance(layer,nn.Conv2d):\n",
    "        #print(layer.in_channels)\n",
    "        lip = lip*computeLayerLipschitzFourier(layer.weight,40)\n",
    "\n",
    "print(lip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'conv2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net\u001b[38;5;241m.\u001b[39mdncnn[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2D\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'conv2D'"
     ]
    }
   ],
   "source": [
    "net.dncnn[0]==nn.conv2D()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sayan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
